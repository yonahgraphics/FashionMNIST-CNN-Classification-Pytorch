{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXrhgTjFG0uz"
   },
   "source": [
    "## WE WILL WORK WITH THE FASHION -MNIST DATASET & WILL FOLLOW THE FOLLOWING PROCESS;\n",
    "#### 1. PREPARE DATA\n",
    "#### 2. BUILD MODEL\n",
    "#### 3. TRAIN MODEL\n",
    "#### 4. ANALYZE THE MODEL'S RESULTS\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBQyzzreFn74"
   },
   "source": [
    "## **1. PREPARE DATA**\n",
    "#### We will use the ETL process to prepare data\n",
    "#### EXTRACT, TRANSFORM, LOAD - ETL process\n",
    "#### Extract the data from the source, transform it into its desirable form, load the data into a suitable structure for querrying and analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "id": "AkR6P_dyAk-q"
   },
   "outputs": [],
   "source": [
    "import torch   ## Top-level pytorch package in the tensor library\n",
    "import torchvision  ## Package that provides access to popular datasets, model architectures and image trasformations for computer vision\n",
    "import torchvision.transforms as transforms ## An interface that gives us acess to common tranformations for our image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BQfpkh1KQTU"
   },
   "source": [
    "#### EXTRACT (Get data from its source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534,
     "referenced_widgets": [
      "bbe76057e82641c5a1ed6f7d904d4751",
      "b665362b662c48bf8167a02a888f5927",
      "988c2ca3586c4f149cf8387c71d8aa2b",
      "8565bdaceafe4951ab39751e7d0a2793",
      "58eeb471f15445c8b3a274b9c3aaaf2f",
      "68c6acfdc844472c9823350064eee16d",
      "583b6c6f7340488e8c85c0eb1fd89d23",
      "ddfdedafa206448a9f4023ee8715fdec",
      "744cd03be9d7438cbc173479aaa6f08f",
      "d687d8fac8ca4d82922f14a81cefad27",
      "74be6a0d694541e983973f44e486610b",
      "88832844280c4fd2a1953651b33069f2",
      "c636de35adb74d0f9325db09301551df",
      "0c778670e477433f8b2af7cfde745da8",
      "4bd0e72cf03d4ef99ff83fc3840c6d80",
      "2416d93c5f0c4437b1218c27aaf8cf17",
      "7ae21dccb1b74efaa75fc0c7f4139972",
      "a42b4a914da742109430f68d14983df5",
      "c1a39a61d3a94bfc9aacba6e54ae41c2",
      "70fda96397bf403197f1d14851f5d956",
      "de47dcbe7cd14f53beb10c068683ee19",
      "b48293faed504d58a1c9a38d6fd8bfef",
      "d3c18772dc724e8ebf714ff686af3155",
      "0ac14e06fc2f4793ad44354e6c709499",
      "21bb443e7ff6459bad933287f03e98d2",
      "3d148d4d57ff4298920daf031b3fa78d",
      "1f42157c5fb44849a78d12bff18b9f7a",
      "6b76d137b75e4344b0b4d49f27fca9b9",
      "f51cee8f47c144d68f4b35240a3339d9",
      "1e81809797e64e5b811e3961e2846655",
      "3f01e2e92edf4ed2a427e6a985e3e921",
      "97302b7800c44da5a26eb685666f4b1c"
     ]
    },
    "id": "6joXzUH0PlMu",
    "outputId": "7f18a29b-fec1-4b96-8741-a1895ce24d1b"
   },
   "outputs": [],
   "source": [
    "## Let's create our fashionMNIST dataset instance\n",
    "\n",
    "    ##==================================================================================================\n",
    "    ##EXTRACT \n",
    "    ##==================================================================================================\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root =  \"./data/FashinMNIST\", ## This tells it where to load the data from if its there\n",
    "    train = True, ## This tells we want the training set\n",
    "    download = True, ## This tells the class to download the data if its not present in the location we specified.\n",
    "    ##==================================================================================================\n",
    "    ##TRANSFORM   (Convert the data into tensor form)\n",
    "    ##==================================================================================================\n",
    "    transform = transforms.Compose([transforms.ToTensor()])) ## Here we pass a composition of transformations that should be performed on the datasets\n",
    "                                                             ## We use ToTensor() transformation since we want our images to be transformed to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B86VligfKRi3"
   },
   "source": [
    "#### LOAD (Put the data into an object to make it easily accessible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "id": "mQD53xm0KN5w"
   },
   "outputs": [],
   "source": [
    " ##==================================================================================================\n",
    " ##LOAD \n",
    " ##==================================================================================================\n",
    "train_loader = torch.utils.data.DataLoader(train_set) ##This wraps the dataset up inside the dataloader object instance\n",
    "                                                      ## DataLoader gives us querrying capabilities\n",
    "                                                      ## Now we can leverage the laoder for tasks that would be otherwise very complicated to implement by hand\n",
    "                                                      ##E.g batchsize, thread management and shuffle capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mA_yYN9nKzM"
   },
   "source": [
    "### PyTorch Datasets and DataLoaders - Training Set Exploration for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "id": "GN9bD7lSnQfo"
   },
   "outputs": [],
   "source": [
    "import torch   ## Top-level pytorch package in the tensor library\n",
    "import torchvision  ## Package that provides access to popular datasets, model architectures and image trasformations for computer vision\n",
    "import torchvision.transforms as transforms ## An interface that gives us acess to common tranformations for our image processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120) ## Sets the linewidth for pytorch output that is printed to the console\n",
    "\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root =  \"./data/FashinMNIST\", ## This tells it where to load the data from if its there\n",
    "    train = True, ## This tells we want the training set\n",
    "    download = True, ## This tells the class to download the data if its not present in the location we specified.\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])) ## Here we pass a composition of transformations that should be performed on the datasets\n",
    "                                                             ## We use ToTensor() transformation since we want our images to be transformed to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "id": "2bRvY_dUoCUD"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root =  \"./data/FashinMNIST\", ## This tells it where to load the data from if its there\n",
    "    train = True, ## This tells we want the training set\n",
    "    download = True, ## This tells the class to download the data if its not present in the location we specified.\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])) ## Here we pass a composition of transformations that should be performed on the datasets\n",
    "                                                             ## We use ToTensor() transformation since we want our images to be transformed to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "id": "Uqzkc2gIoC-X"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 10) ##Here we specify the batch_size so we can look at many images\n",
    "                                                                       ## If we don't, we only get one image as it is the default\n",
    "                                                                       ## So in the ETL process above, we would only get one image                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPvxzhb_pZmc",
    "outputId": "297d1013-020c-43d7-9547-bd1cdb47d918"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set) ## This returns the number of images in the training set, which we already know is 60,000 from Fashion-MNST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yx_OXIOLpe2Q",
    "outputId": "91a5d954-38c5-4ac4-b0a0-22e919924b4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets ##This gives us the target tensor of the dataset\n",
    "                  ##This would have been train_set.train_labels but was changed, now we use targets\n",
    "                  ##Encodes the actual class names or labels, 9 = ankleboot, 0 = T-shirt etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqZDpcos5sKU",
    "outputId": "8a4e2456-2514-4d68-b4a5-838297338625"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.classes ## We can check all the classes in our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdpAwnrR8B_o"
   },
   "source": [
    "### To know all the methods, decorators in train_set, we can run dir(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIGROagD8Plh",
    "outputId": "92017231-c23c-4034-8ab8-08e9d1bb66e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_exists',\n",
       " '_format_transform_repr',\n",
       " '_is_protocol',\n",
       " '_repr_indent',\n",
       " 'class_to_idx',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'download',\n",
       " 'extra_repr',\n",
       " 'mirrors',\n",
       " 'processed_folder',\n",
       " 'raw_folder',\n",
       " 'resources',\n",
       " 'root',\n",
       " 'target_transform',\n",
       " 'targets',\n",
       " 'test_data',\n",
       " 'test_file',\n",
       " 'test_labels',\n",
       " 'train',\n",
       " 'train_data',\n",
       " 'train_labels',\n",
       " 'training_file',\n",
       " 'transform',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJ5hIoHNpofi",
    "outputId": "8c727a58-9ce9-4d5e-d28c-9fc3f5502ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets.bincount()##Here we create bins and count the frquency of occurances within each bin\n",
    "                            ##Calling this on the targets tensor gives the freq distrib of the values in the tensor\n",
    "                            ## The Fashion-MNST dataset is uniform with respect to the number of samples from each class (balanced)\n",
    "                            ## If the classes had varrying number of samples, the dataset would be unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UvobCSB5Itt"
   },
   "source": [
    "### We can make the train_set iterable and iterate over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqJupb-Wp2oD",
    "outputId": "c146b4fc-2d56-4ae4-a1cd-891640265444"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_set))## This returns a single sample\n",
    "len(sample) ##The sample contains two items (image, label) each as a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mc0PBUnO0Vg8",
    "outputId": "8b9c30b1-db96-4624-a9ba-30e5423ab6fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample) ## We can see that each element in the train_set is a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "id": "wvcOWAjz0hb_"
   },
   "outputs": [],
   "source": [
    "# image = sample[0]\n",
    "# label = sample[1]\n",
    "image, label = sample ## We can use squence unpacking to assign the image and the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STTyYoEE3Buw"
   },
   "source": [
    "###### Checking the shapes of the image and label and then plotting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bm-u-bEQ0q15",
    "outputId": "7935b2d8-4aeb-4204-e6b0-d1722b1b854a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape ##This is a grayscale image with a height and width of 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kddlCv633OWf",
    "outputId": "6134fbc0-a3d9-4443-d2b6-f76cf9adc6c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label)## The label is given to us as an int so it does not have a shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "xsiLxk-83Peo",
    "outputId": "b72ba30c-f2cc-4640-eb7a-66f5a2641042"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 9')"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray') ##Here we squeeze off the color channel dimension and also specify the colormap to gray\n",
    "plt.title(\"Label: \" + str(label), fontsize =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jnWTh4z_Snp"
   },
   "source": [
    "## Working with batches and dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MiHagjf4_cmD",
    "outputId": "2fcb2ebc-9fff-4a62-bbaa-9b0d3ed69d76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))##Iteraring over the batch\n",
    "type(batch)##Checking the type if the bacth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp08phfuVBhL",
    "outputId": "4a2f38cb-508d-4d84-aa46-1be337dce245"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch) ##Checking the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "id": "AOzttOMpVENo"
   },
   "outputs": [],
   "source": [
    "images, labels = batch ## upacking the images and labels into the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jj6Ak7DtVIEN",
    "outputId": "0c8d731a-3466-497d-98a3-2b6e83e27122"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape #This shape is 10, 1, 28, 28 because there are 10 grayscale(1) images of height 28 and width 28 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtX-zPvZVJey",
    "outputId": "be74a3e1-dcbe-4b10-d864-0c51e35f2a92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape #This is a tensor containing labels to the ten images in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjaO4CirpPhr",
    "outputId": "3f54f641-0f9d-4150-b4c0-006489a74af3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnIlsfwxkVTk",
    "outputId": "3988d7c6-f3b6-405d-d227-c2a3a3c26234"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "y81UjL9aWHJz",
    "outputId": "d346caed-f66b-44ee-be02-65c8f8fb20c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Labels:    [9, 0, 0, 3, 0, 2, 7, 2, 5, 5]')"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Plotting all the images using the torchvision utility function\n",
    "\n",
    "grid = torchvision.utils.make_grid(images, nrows = 10)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "plt.title(\"Labels:    \" + str(labels.tolist()), fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwwZkCNKoMUJ"
   },
   "source": [
    "## BUILDING A SIMPLE NUERAL NETWORK(SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57MvCK0goW0F",
    "outputId": "24b028c8-a247-4859-be41-3b0bde955ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Within the nn package, there is a class called Module, and it is \n",
    "#the base class for all of neural network modules which includes layers.\n",
    "#This means that all of the layers in PyTorch extend the nn.Module class \n",
    "#and inherit all of PyTorch's built-in functionality within the nn.Module class.\n",
    "#Even neural networks extend the nn.Module class. This makes sense because neural\n",
    "#networks themselves can be thought of as one big layer (if needed, let that sink in over time).\n",
    "#Neural networks and layers in PyTorch extend the nn.Module class. This means that we must extend\n",
    "#the nn.Module class when building a new layer or neural network in PyTorch.\n",
    "\n",
    "## each layer has a transformation (methods/code) and learnable weights (attributes)\n",
    "## All the layers in pytorch extend the nn.Module class\n",
    "## Neural networks and layers can be viewed as the same thing. They are the same object\n",
    "\n",
    "\n",
    "import torch.nn  as nn  ## This is pytorch's neural network library\n",
    "import torch\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1   = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) #out_channels=6 means we want 6 out_channels, which also mean\n",
    "                                                                           # We are setting the number of filters/kernnels to 6 and each will give one out_channel  \n",
    "    #This means we have in_channel/color_channel = 1,\n",
    "    #out_channel/num_kernels/num_filters = 6, kernel size = (5x5)\n",
    "    self.conv2  = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "    self.fc1     = nn.Linear(in_features=12*4*4, out_features=120) #out_features/feature maps\n",
    "    self.fc2     = nn.Linear(in_features=120, out_features=60)\n",
    "    self.out     = nn.Linear(in_features=60, out_features=10) # out_features in the last layer is 10 because we have 10 classes\n",
    "  \n",
    "\n",
    "# PyTorch nn.Modules Have A forward() Method\n",
    "# When we pass a tensor to our network as input, the tensor flows forward though each layer \n",
    "# transformation until the tensor reaches the output layer. This process of a tensor flowing\n",
    "# forward though the network is known as a forward pass.\n",
    "\n",
    "\n",
    "  def forward(self, t):\n",
    " ##Implement the fwd method\n",
    "    return t\n",
    "\n",
    "# Each layer has its own transformation (code) and the tensor passes forward through each layer.\n",
    "# The composition of all the individual layer forward passes defines the overall forward pass transformation for the network.\n",
    "# The goal of the overall transformation is to transform or map the input to the correct prediction \n",
    "# output class, and during the training process, the layer weights (data) are updated in such a way\n",
    "# that cause the mapping to adjust to make the output closer to the correct prediction.\n",
    "\n",
    "my_network = Network()\n",
    "\n",
    "print(my_network) ## When we print the isntance of our network, we get a string representation of our network. \n",
    "                  ## This functionality is inherited from the nn.Module class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ7K7TBd2lVE"
   },
   "source": [
    "## **Convolution opertation:** It performs a dot product of a filter (nxn matrix) and the input. It helps to detect features\n",
    "The ouput has dimensions of (n-f + 1) X (n-f + 1), where n = width/height of the image, f = width/height of the filter\n",
    "\n",
    "### **Zero padding** adds a padding of zeros around an image tensor in order to ensure that we get the same output size as the input without loosing any useful information\n",
    "**-Valid** Means no padding<br>\n",
    "**-Same** Means you want a padding, preserving the image size, hence the name same\n",
    "\n",
    "## **Maxpooling operation:** This operation typically follows a convolution operation <br>\n",
    "### It simply picks the max_value from the input matrix in the area covered by the filter. \n",
    "## Maxpooling reduces dimensions, parameters and overfitting\n",
    "**Stride:** Both maxpooling and convolution ops need a stride, which tell them by how much a filter should be moved over the input matrix to the right and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCUFExqVF-Zp",
    "outputId": "72bfbe56-88eb-4b79-f5f8-f2b94efd8fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn  as nn \n",
    "import torch\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1   = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "   \n",
    "    self.conv2  = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "    self.fc1     = nn.Linear(in_features=12*4*4, out_features=120) #out_features/feature maps\n",
    "    self.fc2     = nn.Linear(in_features=120, out_features=60)\n",
    "    self.out     = nn.Linear(in_features=60, out_features=10) # out_features in the last layer is 10 because we have 10 classes\n",
    "\n",
    "  def forward(self, t):\n",
    " ##Implement the fwd method\n",
    "    return t\n",
    "\n",
    "my_network = Network()\n",
    "\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KTNGLvQLiZs",
    "outputId": "f27ec2d6-0b01-42f1-9962-2e1abab36166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1779, -0.1683,  0.0946,  0.1333,  0.1450],\n",
       "          [-0.1891,  0.1053, -0.1014,  0.1484, -0.0855],\n",
       "          [-0.0047, -0.1934, -0.0700,  0.0430, -0.0626],\n",
       "          [-0.0867, -0.1311, -0.1697,  0.1846, -0.1835],\n",
       "          [ 0.0462,  0.1387,  0.1087,  0.0473,  0.1110]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0302,  0.1202,  0.0641, -0.0827, -0.0659],\n",
       "          [ 0.0062,  0.1276, -0.1399, -0.1458, -0.1724],\n",
       "          [ 0.1204, -0.1728, -0.0752, -0.1296,  0.0138],\n",
       "          [-0.1056,  0.1918, -0.0450,  0.0668,  0.0637],\n",
       "          [-0.0783, -0.0211,  0.1878,  0.0027,  0.0193]]],\n",
       "\n",
       "\n",
       "        [[[-0.0973, -0.0683, -0.1765,  0.0841, -0.1272],\n",
       "          [ 0.0402, -0.1854, -0.0100, -0.0955,  0.0878],\n",
       "          [-0.0983, -0.1561, -0.0212, -0.0642, -0.0443],\n",
       "          [ 0.0851,  0.0635,  0.1178,  0.1570,  0.1902],\n",
       "          [-0.1023, -0.0359,  0.0527, -0.0362, -0.1163]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0203,  0.0647,  0.1049, -0.1219, -0.1070],\n",
       "          [ 0.1030,  0.0922,  0.0910, -0.1936,  0.0915],\n",
       "          [ 0.1417, -0.1742,  0.1328, -0.0143, -0.0198],\n",
       "          [-0.0830, -0.0354,  0.1346,  0.1350,  0.1662],\n",
       "          [-0.1964, -0.0970,  0.0866, -0.0737, -0.0850]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1615, -0.1813,  0.1250,  0.0818, -0.0081],\n",
       "          [-0.1641, -0.0565,  0.0083,  0.0247,  0.0667],\n",
       "          [-0.0416, -0.0864, -0.0030, -0.1155,  0.1224],\n",
       "          [-0.0966, -0.1448, -0.1251,  0.0522,  0.1992],\n",
       "          [ 0.0815, -0.0536, -0.0823, -0.1627, -0.1859]]],\n",
       "\n",
       "\n",
       "        [[[-0.1222, -0.0182, -0.0665, -0.0344,  0.0043],\n",
       "          [ 0.1123,  0.0824, -0.0965, -0.0712,  0.0463],\n",
       "          [-0.0381, -0.0443,  0.1458,  0.1341, -0.1557],\n",
       "          [-0.0424,  0.0164, -0.0810, -0.1526,  0.0412],\n",
       "          [ 0.1842,  0.1224, -0.0156,  0.1798,  0.0089]]]], requires_grad=True)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have access to each of our \n",
    "# layers, we can acess the weights inside each layer \n",
    "\n",
    "my_network.conv1.weight\n",
    "## This is an example of nesting objects. weight object lives in conv1 class, whose object also lives in the network class\n",
    "#The values in this weight tensor are learned as the network trains and learns to reduce the  error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouAlmEwCPRLl"
   },
   "source": [
    "## INVESTIGATING THE SHAPES OF THE WEIGHT TENSORS\n",
    "#### The parameter values that we pass to the layers directly impact our network weights. Let's see this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRidHRsQNdCh",
    "outputId": "28912999-1e06-4376-fb36-7409abebfc0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.conv1.weight.shape # This means we have 6 filters of size 5x5, convolving 1 input channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tNdHd80PDnr",
    "outputId": "61d1b026-36e8-4967-9023-bf319f74927e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 5, 5])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNwsjTTaPdOm",
    "outputId": "9d267710-8656-4a44-a4b5-544f5b547ed2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0Y5kcS4Pf93",
    "outputId": "41fb4ff3-dc42-45b7-aafc-d371eeb13de2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 120])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.fc2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7V6nEChqPhUc",
    "outputId": "58774609-e7d1-4597-a86a-d5a29ce5acf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.out.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozfQ_K8pR0Rn",
    "outputId": "0849e72f-cc08-4dc7-fc11-0d1a6d1b6718"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 5])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.conv2.weight[0].shape ## We can index out a single filter by indexing the first axis of the weight tensor. \n",
    "                                ## This gives a filter of width and height 5 and depth 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYJxWZdNT9yD"
   },
   "source": [
    "## weight tensors for linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0ZsDTP-T7xo",
    "outputId": "924efadf-b5c3-4cb4-da1c-467a5635e9c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.fc1.weight.shape ## These are rank 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwYdnDsxUGKZ",
    "outputId": "7a21c6f1-eec5-4a6a-a5d6-9a0794e86a24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 120])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.fc2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPR6bwo1UJT4",
    "outputId": "f23c34fa-dbd5-4b38-e209-e87bf1af4e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network.out.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGAvtuKEj2n3"
   },
   "source": [
    "## Maxtrix multiplication using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "id": "2CRHz8G1ULPu"
   },
   "outputs": [],
   "source": [
    "in_features = torch.tensor([1,2,3,4], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3IDx7xskE4X",
    "outputId": "a6fcccc8-995b-4480-e663-83fcb8c07978"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "id": "7UTP2GRVkPlp"
   },
   "outputs": [],
   "source": [
    "weight_matrix = torch.tensor([\n",
    "                              [1,2,3, 4],\n",
    "                              [2,3,4,5],\n",
    "                              [3,4,5,6],\n",
    "                              ], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajGHQo7S9uAV",
    "outputId": "a1f0ac67-95a6-4b0b-b08c-c4bd705fa74d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [2., 3., 4., 5.],\n",
       "        [3., 4., 5., 6.]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-JKN9UHkuiN",
    "outputId": "fab32d16-8361-42bd-9b87-c6f6fcad9f73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULODFgj4loPC",
    "outputId": "2b5fa19a-1efa-41d0-85aa-768000879964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45F06rXoI76i",
    "outputId": "86695310-28f4-4af8-93cb-b1031ace95b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWFZTQQ0k3ih",
    "outputId": "2751f615-dbb2-4a2a-ad80-d0e3a6031ff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-NLYJEnlbp9",
    "outputId": "5be8359b-8e48-4668-db34-6d66221c8af1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(weight_matrix,in_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-zOuChVFsUC"
   },
   "source": [
    "## Accessing all parameters at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzvbpXVeFnlt",
    "outputId": "68ac7213-1e85-48b6-87bf-94881dca4802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([12, 6, 5, 5])\n",
      "torch.Size([12])\n",
      "torch.Size([120, 192])\n",
      "torch.Size([120])\n",
      "torch.Size([60, 120])\n",
      "torch.Size([60])\n",
      "torch.Size([10, 60])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in my_network.parameters():\n",
    "  print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmfRwtpJF5Rs",
    "outputId": "d5c2bd50-396c-446c-e46c-c83c1b9578fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in my_network.named_parameters():\n",
    "  print(name, '\\t\\t', param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIqgcNy9dVOP"
   },
   "source": [
    "##Transform Using A Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVuYagLYGyfZ",
    "outputId": "1180876d-e608-46ab-9070-0c75e6aa4ecd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "\n",
    "weight_matrix = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "weight_matrix.matmul(in_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXDtFUTVm3nE",
    "outputId": "f71a825d-bd98-4381-c0a7-ed121f7a8c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(in_features.shape)\n",
    "print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SfTg9zxjsM1",
    "outputId": "5da01058-73b0-424b-c719-6fd2c1b707a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Here since the input tensor is on the left side, we have to transpose the weight tensor before performing the matplication.\n",
    "#### That's what happens under the hood in the Linear layer\n",
    "in_features.matmul(weight_matrix.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F28A4tEvdkVm"
   },
   "source": [
    "\n",
    "Here, we have created a 1-dimensional tensor called in_features. We have also created a weight matrix which of course is a 2-dimensional tensor. Then, we've use the matmul() function to preform the matrix multiplication operation that produces a 1-dimensional tensor.\n",
    "\n",
    "In general, the weight matrix defines a linear function that maps a 1-dimensional tensor with four elements to a 1-dimensional tensor that has three elements. We can think of this function as a mapping from 4-dimensional Euclidean space to 3-dimensional Euclidean space.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v22nA0rMdZKd",
    "outputId": "4de476b8-6d58-45cc-d7c4-07b63cf817f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=3, bias=False)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how to create a PyTorch linear layer that will do this same operation.\n",
    "\n",
    "fc = nn.Linear(in_features=4, out_features=3, bias=False)\n",
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "id": "WvZuMgq4d3Zt"
   },
   "outputs": [],
   "source": [
    "## ??nn.Linear() Use this to see that pytorch creates  a weight matrix of shape (out_features X in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJ5HEJppeN5X",
    "outputId": "bf1c503d-3ff1-4b4a-8dc6-021c87f0cf35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8309, -2.0490, -0.3759], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)## The weight matrix is initialized with random values so we dnt expect the output herento match the one we got when we did used matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WighuSugIop"
   },
   "source": [
    "\n",
    "## Let's explicitly set the weight matrix of the linear layer to be the same as the one we used in our other example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Erp622_NfWhj",
    "outputId": "8649ef8b-2f8c-467c-d589-935c7fe4391d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.weight = nn.Parameter(weight_matrix) \n",
    "fc(in_features) ## Now we get the same output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkHi1l95hA5t"
   },
   "source": [
    "# Callable Layers And Neural Networks\n",
    "We pointed out before how it was kind of strange that we called the layer object instance as if it were a function.\n",
    "\n",
    "> fc(in_features)\n",
    "tensor([30.0261, 40.1404, 49.7643], grad_fn=<AddBackward0>)\n",
    "What makes this possible is that PyTorch module classes implement another special Python function called __call__(). If a class implements the __call__() method, the special call method will be invoked anytime the object instance is called.\n",
    "\n",
    "This fact is an important PyTorch concept because of the way the __call__() method interacts with the forward() method for our layers and networks.\n",
    "\n",
    "alling the forward() method directly, we call the object instance. After the object instance is called, the __call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. This applies to all PyTorch neural network modules, namely, networks and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "phchG9VZgRLT",
    "outputId": "ad337e73-7c93-480d-8365-5e047492e118"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n __call__ : Callable[..., Any] = _call_impl\\n'"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##??nn.Module\n",
    "\n",
    "'''\n",
    " __call__ : Callable[..., Any] = _call_impl\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIRtFQXsk4kE"
   },
   "source": [
    "### The extra code that PyTorch runs inside the __call__() method is why we never invoke the forward() method directly. If we did, the additional PyTorch code would not be executed. As a result, any time we want to invoke our forward() method, we call the object instance. This applies to both layers, and networks because they are both PyTorch neural network modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8l_CFIgvozsB"
   },
   "source": [
    "## FORWARD METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "6iRVZ-bQhdtC",
    "outputId": "4aa7dffe-6b08-4d18-c0cc-253470466f7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nAs we can see here, our input tensor is transformed as we move through the convolutional layers. \\nThe first convolutional layer has a convolutional operation, followed by a relu activation operation whose\\noutput is then passed to a max pooling operation with kernel_size=2 and stride=2.\\n\\nThe output tensor t of the first convolutional layer is then passed to the next convolutional layer,\\n which is identical except for the fact that we call self.conv2() instead of self.conv1().\\n\\nEach of these layers is comprised of a collection of weights (data) and a collection operations (code). \\nThe weights are encapsulated inside the nn.Conv2d() class instance. The relu() and the max_pool2d() calls are\\n just pure operations. Neither of these have weights, and this is why we call them directly from the nn.functional API.\\n\\n\\n Sometimes we may see pooling operations referred to as pooling layers. Sometimes we may even hear activation operations called activation layers.\\n\\nHowever, what makes a layer distinct from an operation is that layers have weights. Since pooling operations and activation functions do not have weights,\\n we will refer to them as operations and view them as being added to the collection of layer operations.\\n\""
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "      # (1) input layer\n",
    "      t = t \n",
    "\n",
    "      # (2) hidden conv layer\n",
    "      t = self.conv1(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (3) hidden conv layer\n",
    "      t = self.conv2(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (4) hidden linear layer\n",
    "      t = t.reshape(-1, 12 * 4 * 4)\n",
    "      t = self.fc1(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (5) hidden linear layer\n",
    "      t = self.fc2(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (6) output layer\n",
    "      t = self.out(t)\n",
    "      #t = F.softmax(t, dim=1)\n",
    "      '''\n",
    "      Inside the network we usually use relu() as our non-linear activation function,\n",
    "      but for the output layer, whenever we have a single category that we are trying to \n",
    "      predict, we use softmax(). The softmax function returns a positive probability for\n",
    "      each of the prediction classes, and the probabilities sum to 1.\n",
    "      However, in our case, we won't use softmax() because the loss function that we'll use,\n",
    "      F.cross_entropy(), implicitly performs the softmax() operation on its input, so we'll \n",
    "      just return the result of the last linear transformation\n",
    "      '''\n",
    "      return t\n",
    "\n",
    "''''\n",
    "As we can see here, our input tensor is transformed as we move through the convolutional layers. \n",
    "The first convolutional layer has a convolutional operation, followed by a relu activation operation whose\n",
    "output is then passed to a max pooling operation with kernel_size=2 and stride=2.\n",
    "\n",
    "The output tensor t of the first convolutional layer is then passed to the next convolutional layer,\n",
    " which is identical except for the fact that we call self.conv2() instead of self.conv1().\n",
    "\n",
    "Each of these layers is comprised of a collection of weights (data) and a collection operations (code). \n",
    "The weights are encapsulated inside the nn.Conv2d() class instance. The relu() and the max_pool2d() calls are\n",
    " just pure operations. Neither of these have weights, and this is why we call them directly from the nn.functional API.\n",
    "\n",
    "\n",
    " Sometimes we may see pooling operations referred to as pooling layers. Sometimes we may even hear activation operations called activation layers.\n",
    "\n",
    "However, what makes a layer distinct from an operation is that layers have weights. Since pooling operations and activation functions do not have weights,\n",
    " we will refer to them as operations and view them as being added to the collection of layer operations.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enFcx3qisuB0"
   },
   "source": [
    "## CNN Image Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "id": "VI0fSGi-pPv8"
   },
   "outputs": [],
   "source": [
    "import torch   ## Top-level pytorch package in the tensor library\n",
    "import torchvision  ## Package that provides access to popular datasets, model architectures and image trasformations for computer vision\n",
    "import torchvision.transforms as transforms ## An interface that gives us acess to common tranformations for our image processing\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120) ## Sets the linewidth for pytorch output that is printed to the console\n",
    "\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root =  \"./data/FashinMNIST\", ## This tells it where to load the data from if its there\n",
    "    train = True, ## This tells we want the training set\n",
    "    download = True, ## This tells the class to download the data if its not present in the location we specified.\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])) ## Here we pass a composition of transformations that should be performed on the datasets\n",
    "                                                             ## We use ToTensor() transformation since we want our images to be transformed to tensors\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "      # (1) input layer\n",
    "      t = t \n",
    "\n",
    "      # (2) hidden conv layer\n",
    "      t = self.conv1(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (3) hidden conv layer\n",
    "      t = self.conv2(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (4) hidden linear layer\n",
    "      t = t.reshape(-1, 12 * 4 * 4) # This means we want to reshape to (1x192).\n",
    "                                    # This flattens each image tensor in the batch\n",
    "                                    #This is a tensor of a batch of 1 and a flattened image pf 192 pixels\n",
    "      t = self.fc1(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (5) hidden linear layer\n",
    "      t = self.fc2(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (6) output layer\n",
    "      t = self.out(t)\n",
    "  \n",
    "      return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l1771KMwVSx"
   },
   "source": [
    "Before we being, we are going to turn off PyTorch's gradient calculation feature. This will stop PyTorch from automatically building a computation graph as our tensor flows through the network.\n",
    "\n",
    "he computation graph keeps track of the network's mapping by tracking each computation that happens. The graph is used during the training process to calculate the derivative (gradient) of the loss function with respect to the network's weights.\n",
    "\n",
    "Since we are not training the network yet, we aren't planning on updating the weights, and so we don't require gradient calculations. We will turn this back on when training begins.\n",
    "\n",
    "This process of tracking calculations happens in real-time, as the calculations occur. Remember back at the beginning of the series, we said that PyTorch uses a dynamic computational graph. We'll now we're turning it off.\n",
    "\n",
    "Turning it off isn't strictly necessary but having the feature turned off does reduce memory consumption since the graph isn't stored in memory. This code will turn the feature off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "id": "CaJUFe5hvZ9l"
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False) \n",
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGpqRi4Nw0t7",
    "outputId": "09b719c2-9abe-4ac1-b0fc-a0c7d5717ac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next, we'll procure a single sample from our training set, unpack the image and the label, and verify the image's shape:\n",
    "\n",
    "sample = next(iter(train_set)) \n",
    "image, label = sample \n",
    "image.shape \n",
    "torch.Size([1, 28, 28])\n",
    "#The image tensor's shape indicates that we have a single channel image that is 28 in height and 28 in width. Cool, this is what we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73_COmwmxKDD"
   },
   "source": [
    "Now, there's a second step we must preform before simply passing this tensor to our network. When we pass a tensor to our network, the network is expecting a batch, so even if we want to pass a single image, we still need a batch.\n",
    "\n",
    "This is no problem. We can create a batch that contains a single image. All of this will be packaged into a single four dimensional tensor that reflects the following dimensions.\n",
    "\n",
    "## (batch_size, in_channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dc8vjREUxCFf",
    "outputId": "4165463b-fa23-4231-d715-652239edb8d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserts an additional dimension that represents a batch of size 1\n",
    "image.unsqueeze(0).shape\n",
    "torch.Size([1, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7S3AxdgxTwl",
    "outputId": "458912c0-160c-474a-967d-1b5b8611b00d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0270,  0.0332, -0.0108, -0.0501, -0.0041,  0.0319,  0.1046, -0.0017, -0.0940,  0.0396]])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pred = network(image.unsqueeze(0)) # image shape needs to be (batch_size × in_channels × H × W)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB1m4Mr2yTts",
    "outputId": "33f3c206-849d-420e-bff7-dee00e6fba04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjoEZNJQzAnt",
    "outputId": "c3c8815c-9475-48d1-f856-6fe2dc19d10b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label ##Correct label of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSITaG_AzGxY",
    "outputId": "12c5858c-a522-4c93-fa41-63c4f1a276bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7QPswc_0HGP"
   },
   "source": [
    "The shape of the prediction tensor is 1 x 10. This tells us that the first axis has a length of one while the second axis has a length of ten. The interpretation of this is that we have one image in our batch and ten prediction classes.\n",
    "\n",
    "**(batch size, number of prediction classes)**\n",
    "\n",
    "\n",
    "\n",
    "For each input in the batch, and for each prediction class, we have a prediction value. If we wanted these values to be probabilities, we could just the softmax() function from the nn.functional package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVUl8c9ozJlB",
    "outputId": "26ed624b-99da-46bb-9a2a-bd7a6d2b8100"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1018, 0.1025, 0.0981, 0.0943, 0.0987, 0.1023, 0.1100, 0.0989, 0.0902, 0.1031]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import random\n",
    "# random.seed(4)\n",
    "F.softmax(pred, dim=1) ## transform the predictions into probabilities\n",
    "\n",
    "## Please note that the output generated will be different each time we run the notebook since the network weights are randomly generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leXWlcsy0W_E",
    "outputId": "0e26494c-4ed2-4860-ca9d-2851d64d588a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The sum of the probabilities must be 1\n",
    "F.softmax(pred, dim=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hy1aEOBE0aCz",
    "outputId": "6f95fc69-834f-49f4-807d-6eb06681c8a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(pred, dim=1).argmax() ## Here let's get the index that produces the highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vlPkEyu0zhi"
   },
   "source": [
    "The label for the first image in our training set is 9, and using the argmax() function we can see that the highest value in our prediction tensor occurred at the class represented by index 7.\n",
    "\n",
    "**Prediction: Sneaker (7) Note that this can change**<br>\n",
    "**Actual: Ankle boot (9)**\n",
    "\n",
    "The prediction in this case is incorrect, which is what we expect because the weights in the network were generated randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoYf7qnm0ofR",
    "outputId": "c03859f2-25c9-4207-a256-761ddc045a1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.classes ## Take a look at the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pqycuqN6F5yA",
    "outputId": "9add4fee-e8c2-4d62-af27-19b074e87bb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shirt'"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.classes[pred.argmax(dim=1)] ## Take a look at the predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRiD9x4Q1toK"
   },
   "source": [
    "## Network Weights Are Randomly Generated\n",
    "There are a couple of important things we need to point out about these results. Most of the probabilities came in close to 10%, and this makes sense because our network is guessing and we have ten prediction classes coming from a balanced dataset.\n",
    "\n",
    "Another implication of the randomly generated weights is that each time we create a new instance of our network, the weights within the network will be different. This means that the predictions we get will be different if we create different networks. Keep this in mind. Your predictions will be different from what we see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrrMVpLa1xeQ",
    "outputId": "120189d8-59c7-4b4e-b4c0-969a6a9515ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0541,  0.0866, -0.1236, -0.0565, -0.0530, -0.0768,  0.0570, -0.1285, -0.1615,  0.0330]])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Network()\n",
    "net2 = Network()\n",
    "\n",
    "net1(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOxrJOkn15Td",
    "outputId": "32ef3828-ec33-4aed-d39b-60207dab2b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0286,  0.0407,  0.0371, -0.0868,  0.1917,  0.0630, -0.0204, -0.0527,  0.0141,  0.0575]])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyazKwXbNPcG"
   },
   "source": [
    "## Passing A Batch Of Images To The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "id": "C2Xo7id0NSF7"
   },
   "outputs": [],
   "source": [
    "import torch   ## Top-level pytorch package in the tensor library\n",
    "import torchvision  ## Package that provides access to popular datasets, model architectures and image trasformations for computer vision\n",
    "import torchvision.transforms as transforms ## An interface that gives us acess to common tranformations for our image processing\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120) ## Sets the linewidth for pytorch output that is printed to the console\n",
    "\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root =  \"./data/FashinMNIST\", ## This tells it where to load the data from if its there\n",
    "    train = True, ## This tells we want the training set\n",
    "    download = True, ## This tells the class to download the data if its not present in the location we specified.\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])) ## Here we pass a composition of transformations that should be performed on the datasets\n",
    "                                                             ## We use ToTensor() transformation since we want our images to be transformed to tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0YtrF_XOLx3"
   },
   "source": [
    "#### Now, we'll use our training set to create a new DataLoader instance, and we'll set our batch_size=10, so the outputs will be more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "id": "7UEg4RL_OABn"
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "id": "MZJTEoiCOZKE"
   },
   "outputs": [],
   "source": [
    "# We'll pull a batch from the data loader and unpack the image and label tensors \n",
    "# from the batch. We'll name our variables using the plural forms since we know the \n",
    "# data loader is returning a batch of ten images when we call next on the data loader iterator.\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "images, labels = batch\n",
    "# In the last episode, when we pulled a single image from our training set,\n",
    "# we had to unsqueeze() the tensor to add another dimension that would effectively transform the singleton image into a batch with a size of one. \n",
    "# Now that we are working with the data loader, we are dealing with\n",
    "# batches by default, so there is no further processing needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJ39r5P0OoDD",
    "outputId": "6ec0644e-7cce-4a72-9c07-74d68728eedd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alright. Let's get a prediction by passing the images tensor to the network.\n",
    "\n",
    "preds = network(images)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6tvopWYR4VU"
   },
   "source": [
    "##### The prediction tensor has a shape of 10 by 10, which gives us two axes that each have a length of ten.\n",
    "#####  This reflects the fact that we have ten images and for each of these ten images we have ten prediction classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4pIs_RxRF6U"
   },
   "source": [
    "## Using Argmax: Prediction Vs Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_Ja8ijIRHRA",
    "outputId": "491ec8e8-3751-4071-edfd-5b2b194a6286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1) ## Predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9E-82nGuReR2",
    "outputId": "42e209f4-3703-44f3-b728-8498a0d787bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels ## Actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcHnT8rNS9FE"
   },
   "source": [
    "## Comparing to the label tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1A0BOYHSltF",
    "outputId": "380b84a9-4516-43e8-b55e-faca6aaa7518"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Actual labels\n",
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0u4hpAKbTGlU",
    "outputId": "61d3269b-1431-422d-d6f6-dc7b4499910d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Total number of correct predictions \n",
    "# if we call the sum() function on this result, we can reduce the output into\n",
    "#  a single number of correct predictions inside this scalar valued tensor\n",
    "\n",
    "preds.argmax(dim=1).eq(labels).sum()\n",
    "\n",
    "# We can see that the network performs poorly since it is not trained yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Etzha4cpVgb9"
   },
   "source": [
    "#### We can wrap this last call into a function called get_num_correct() that accepts the predictions and the labels, and uses the item() method to return the Python number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "id": "AvHQL17vTlBR"
   },
   "outputs": [],
   "source": [
    "def get_num_correct_pred(preds:torch.float32,labels:torch.float32):\n",
    "  predicted_labels = preds.argmax(dim = 1)\n",
    "  comparision = predicted_labels.eq(labels)\n",
    "  num_correct_pred = comparision.sum().item()\n",
    "  return num_correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqU7joLAVXIU",
    "outputId": "c2696e80-a346-40bd-ae5a-95f06db5e241"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Number of correct predictions\n",
    "get_num_correct_pred(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6PqZDYfXjvA"
   },
   "source": [
    "## CNN OUTPUT SIZE FORMULA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVpgsP4aVa5B",
    "outputId": "47199e10-aa25-4287-e512-7ecbd08b0c27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0571, -0.0002,  0.0227,  0.1459,  0.0443,  0.0893,  0.0299, -0.1398,  0.0337,  0.0685]])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing A Batch Of Size One (A Single Image)\n",
    "# In a previous episode, we saw how we can pass a single image by adding a batch dimension using\n",
    "# PyTorch's unsqueeze() method. We'll pass this tensor to the network again, but this time we will\n",
    "# step through the forward() method using the debugger. This will allow us to inspect our tensor as transformations are performed.\n",
    "\n",
    "network = Network()\n",
    "network(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsGrYsIMfbSI"
   },
   "source": [
    "# THIS CODE IS FROM THE DEBUGGER IN VSCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "id": "SmbbmV0ufGM3"
   },
   "outputs": [],
   "source": [
    "# # We saw how we can pass a single image by adding a batch dimension using PyTorch's unsqueeze() method.\n",
    "# #  We'll pass this tensor to the network again, but this time we will step through the forward() method using the debugger.\n",
    "# #   This will allow us to inspect our tensor as transformations are performed.\n",
    "\n",
    "\n",
    "# network = Network()\n",
    "# network(image.unsqueeze(0))\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# #1 Input Layer\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# When the tensor comes into the input layer, we have:\n",
    "\n",
    "# > t.shape\n",
    "# torch.Size([1, 1, 28, 28])\n",
    "# This value in each of these dimensions represent the following values: (batch_size, color_channel, height, width)\n",
    "\n",
    "\n",
    "# #-------------------------------------------------------------------\n",
    "# #2 Convolutional Layer (1)\n",
    "# #-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# When the tensor comes into this layer, we have:\n",
    "\n",
    "# > t.shape\n",
    "# torch.Size([1, 1, 28, 28])\n",
    "# After the first convolution operation self.conv1, we have:\n",
    "\n",
    "# > t.shape\n",
    "# torch.Size([1, 6, 24, 24])\n",
    "# The batch size is still 1. This makes sense because we wouldn't expect our batch size to change, and this is going to be the case through the entire forward pass.\n",
    "\n",
    "# The batch_size is fixed as we move through the forward pass.\n",
    "\n",
    "# The number of color channels has increased from 1 to 6. After we move forward beyond the first convolutional layer, \n",
    "# we don't think of the channels as color channels any longer. We just think of them as output channels. \n",
    "# The reason we have 6 output channels is due to the number of out_channels that we specified when self.conv1 was created.\n",
    "\n",
    "\n",
    "# #------------------------------------------------------------------------\n",
    "# Convolution Operations Use Filters\n",
    "# #------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Like we have seen, this number 6 is arbitrary. The out_channels parameter instructs the nn.Conv2d layer class generate six filters, also known as kernels, with shape 5 by 5 with randomly initialized values. These filters are used to generate the six output channels.\n",
    "\n",
    "# The out_channels parameter determines how many filters will be created.\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# The algorithm:\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Color channels are passed in.\n",
    "# Convolutions are performed using the weight tensor (filters).\n",
    "# Feature maps are produced and passed forward\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "# The Max Pooling Operation\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# The pooling operation reduces the shape of our tensor further by extracting the maximum value from each 2x2 location within our tensor.\n",
    "\n",
    "# > t.shape\n",
    "# torch.Size([1, 6, 24, 24])\n",
    "\n",
    "# > t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "# > t.shape\n",
    "# torch.Size([1, 6, 12, 12])\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "# Convolution Layer Summary\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# The shapes of the tensor input to and output from the convolutional layer is given by:\n",
    "# Input shape: [1, 1, 28, 28]\n",
    "# Output shape: [1, 6, 12, 12]\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "# Summary of each operation that occurs:\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# The convolution layer convolves the input tensor using six randomly initialized 5x5 filters.\n",
    "# This reduces the height and width dimensions by four.\n",
    "# The relu activation function operation maps all negative values to 0.\n",
    "# This means that all the values in the tensor are now positive.\n",
    "# The max pooling operation extracts the max value from each 2x2 section of the six feature maps that were created by the convolutions.\n",
    "# This reduced the height and width dimensions by twelve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0c_4phKl9u8"
   },
   "source": [
    "## CNN Output Size Formula (SQUARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "id": "liGtZ-xqmBS8"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the formula for computing the output size of the tensor after performing convolutional and pooling operations. <br>\n",
    "      # CNN Output Size Formula (Square)\n",
    "      # Suppose we have an nxn input.\n",
    "      # Suppose we have an fxf filter.\n",
    "      # Suppose we have a padding of p and a stride of s.\n",
    "# The output size O is given by this formula:\n",
    "\n",
    "\n",
    "#O = [(n-f+2p)/s] + 1\n",
    "\n",
    "\n",
    "# This value will be the height and width of the output.\n",
    "# However, if the input or the filter isn't a square, this\n",
    "# formula needs to be applied twice, once for the width and once for the height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmTOFK_SnU24"
   },
   "source": [
    "## CNN Output Size Formula (NON-SQUARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "id": "a3Gmd-PbnZgm"
   },
   "outputs": [],
   "source": [
    "# Suppose we have an nh x nw input.\n",
    "# Suppose we have an fh x fw filter.\n",
    "# Suppose we have a padding of p and a stride of s.\n",
    "# The output size O is given by this formula:\n",
    "\n",
    "\n",
    "#Oh  = [(nh-fh+2p)/s] + 1\n",
    "\n",
    "#Ow  = [(nw-fw+2p)/s] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "id": "Ql22YVr6n-f_"
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "# #3 Convolutional Layer (2)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# The second hidden convolutional layer self.conv2, transforms the \n",
    "#tensor in the same was as self.conv1 and reduces the height and width dimensions further.\n",
    "#Before we run through these transformations, let's check the shape of the weight tensor for self.conv2:\n",
    "\n",
    "# self.conv2.weight.shape\n",
    "# torch.Size([12, 6, 5, 5])\n",
    "# This time our weight tensor has twelve filters of height of five and width of five,\n",
    "# but instead of having a single input channel, the number of channels is is coming in at six, which gives the filters a depth. \n",
    "#This accounts for the six output channels from the first convolutional layer. The resulting output will have twelve channels.\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Let's run these operations now.\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# > t.shape\n",
    "# torch.Size([1, 6, 12, 12])\n",
    "\n",
    "# > t = self.conv2(t)\n",
    "# > t.shape\n",
    "# torch.Size([1, 12, 8, 8])\n",
    "\n",
    "# > t.min().item()\n",
    "# -0.39324113726615906\n",
    "\n",
    "# > t = F.relu(t)\n",
    "# > t.min().item()\n",
    "# 0.0\n",
    "\n",
    "# > t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "# > t.shape\n",
    "# torch.Size([1, 12, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "# The shape of the resulting output of self.conv2 allows us to\n",
    "# see why we reshape the tensor using 12*4*4 before passing the tensor to the first linear layer, self.fc1.\n",
    "\n",
    "# As we have seen in the past, this particular reshaping is called flattening the tensor.\n",
    "# The flatten operation puts all of the tensor's elements into a single dimension.\n",
    "\n",
    "# > t = t.reshape(-1, 12*4*4)\n",
    "# > t.shape\n",
    "# torch.Size([1, 192])\n",
    "# The resulting shape is 1x192. The 1 in this case represents the batch size, and \n",
    "#the 192 is the number of elements in the tensor that are now in the same dimension.\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# #4 #5 #6 Linear Layers\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# Now, we just have a series of linear layers followed by non-linear activation function until we reach the output layer.\n",
    "\n",
    "# > t = self.fc1(t)\n",
    "\n",
    "# > t.shape\n",
    "# torch.Size([1, 120])\n",
    "\n",
    "# > t = self.fc2(t)\n",
    "# > t.shape\n",
    "# torch.Size([1, 60])\n",
    "\n",
    "# > t = self.out(t)\n",
    "# > t.shape\n",
    "# torch.Size([1, 10])\n",
    "\n",
    "# > t\n",
    "# tensor([[ 0.1009, -0.0842,  0.0349, -0.0640,  0.0754, -0.0057,  0.0878,  0.0296,  0.0345,  0.0236]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "id": "v_dIR9LMfRuX"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "                  #SUMMARY\n",
    "#--------------------------------------------------------\n",
    "\n",
    "# Operation\tOutput Shape\n",
    "# Identity function\ttorch.Size([1, 1, 28, 28])\n",
    "# Convolution (5 x 5)     \ttorch.Size([1, 6, 24, 24]) ## We get this because we dnt use any padding and stride = 1 1.e. O = (28 - 5 + 0)/1 + 1 = 24\n",
    "# Max pooling (2 x 2)     \ttorch.Size([1, 6, 12, 12]) ## For maxpooling, we get almost the same, but the stride here is 2, so we get 12 i.e O = (24 - 2 + 0)/2  + 1 = 12\n",
    "# Convolution (5 x 5)     \ttorch.Size([1, 12, 8, 8])\n",
    "# Max pooling (2 x 2)     \ttorch.Size([1, 12, 4, 4])\n",
    "# Flatten (reshape)\t        torch.Size([1, 192])\n",
    "# Linear transformation    \ttorch.Size([1, 120])\n",
    "# Linear transformation\t    torch.Size([1, 60])\n",
    "# Linear transformation\t    torch.Size([1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41EIEGzGB4wL"
   },
   "source": [
    "# TRAINING A CNN, CALCULATE THE LOSS, GRADIENT AND UPDATE THE WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mPeAqO1E3Bi"
   },
   "source": [
    "## Training: What We Do After The Forward Pass <br>\n",
    "During training, we do a forward pass, but then what? We'll suppose we get a batch and pass it forward through the network. Once the output is obtained, we compare the predicted output to the actual labels, and once we know how close the predicted values are from the actual labels, we tweak the weights inside the network in such a way that the values the network predicts move closer to the true values (labels).\n",
    "\n",
    "All of this is for a single batch, and we repeat this process for every batch until we have covered every sample in our training set. After we've completed this process for all of the batches and passed over every sample in our training set, we say that an epoch is complete. We use the word epoch to represent a time period in which our entire training set has been covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "id": "M1M-JU0Fhi6m"
   },
   "outputs": [],
   "source": [
    "# During the entire training process, we do as many epochs as necessary to\n",
    "#  reach our desired level of accuracy. With this, we have the following steps:\n",
    "    # Get batch from the training set.\n",
    "    # Pass batch to network.\n",
    "    # Calculate the loss (difference between the predicted values and the true values).\n",
    "    # Calculate the gradient of the loss function w.r.t the network's weights.\n",
    "    # Update the weights using the gradients to reduce the loss.\n",
    "    # Repeat steps 1-5 until one epoch is completed.\n",
    "    # Repeat steps 1-6 for as many epochs required to reach the minimum loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7Wa2WVpJbGT",
    "outputId": "068e84f9-7a0a-414c-c817-8f3b22eed73c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x216296116d0>"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "id": "budpF7tCKI8U"
   },
   "outputs": [],
   "source": [
    "def get_num_correct_pred(preds,labels):\n",
    "  predicted_labels = preds.argmax(dim = 1)\n",
    "  comparision = predicted_labels.eq(labels)\n",
    "  num_correct_pred = comparision.sum().item()\n",
    "  return num_correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "id": "1hweoIm4LE_S"
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root =  \"./data/FashinMNIST\", ## This tells it where to load the data from if its there\n",
    "    train = True, ## This tells we want the training set\n",
    "    download = True, ## This tells the class to download the data if its not present in the location we specified.\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])) ## Here we pass a composition of transformations that should be performed on the datasets\n",
    "                                                             ## We use ToTensor() transformation since we want our images to be transformed to tensors\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "      # (1) input layer\n",
    "      t = t \n",
    "\n",
    "      # (2) hidden conv layer\n",
    "      t = self.conv1(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (3) hidden conv layer\n",
    "      t = self.conv2(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (4) hidden linear layer\n",
    "      t = t.reshape(-1, 12 * 4 * 4) # This means we want to reshape to (1x192).\n",
    "                                    # This flattens each image tensor in the batch\n",
    "                                    #This is a tensor of a batch of 1 and a flattened image pf 192 pixels\n",
    "      t = self.fc1(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (5) hidden linear layer\n",
    "      t = self.fc2(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (6) output layer\n",
    "      t = self.out(t)\n",
    "  \n",
    "      return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY4kUK9SLrUb"
   },
   "source": [
    "## Creating an instance of a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "id": "Gjcff43jKwYy"
   },
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WKfthHuLoOt"
   },
   "source": [
    "## Getting a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "id": "Ue1RbUfDKkbH"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "batch = next(iter(train_loader)) # Getting a batch\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6CoZD8bMF_r"
   },
   "source": [
    "## Calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uuCvNntLgLf",
    "outputId": "147ce779-6ed9-462e-8f54-fccdfd1f0a85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.311347723007202"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds  = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRTRdoUaMwYn"
   },
   "source": [
    "## Calculating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3OeiG_ONHxR",
    "outputId": "7fc76ee9-622c-4c11-d8c9-8edcef60a90c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad) #Initially no gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "id": "3R78KxuRMPJZ"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGWdf2F9M7oj",
    "outputId": "4937522c-c176-48d8-bac8-ccd8f291736c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad.shape #Now the gradients have been calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a6IS4ciOOMK"
   },
   "source": [
    "## Updating The Weights\n",
    "hese gradients are used by the optimizer to update the respective weights. To create our optimizer, we use the torch.optim package that has many optimization algorithm implementations that we can use e.g SGD and Adam. We'll use Adam for our example\n",
    "\n",
    "To the Adam class constructor, we pass the network parameters (this is how the optimizer is able to access the gradients), and we pass the learning rate .\n",
    "\n",
    "Finally, all we have to do to update the weights is to tell the optimizer to use the gradients to step in the direction of the loss function's minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMvRo1FVOmRk",
    "outputId": "704134ce-e471-45d8-afd3-54c24218eaeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.311347723007202"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item() # Checking loss again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn5x72GoPK0v",
    "outputId": "ab6ffc13-046b-49b0-cd13-4bda60738f64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct_pred(preds, labels) ## Correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "id": "orzivZ2MPTkG"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "optimizer.step() # Updating the weights, telling the optimizer to start stepping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP10t-7PSMAk"
   },
   "source": [
    "# Now if we check the loss and number of correct predictions, we see an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Xk0yFfuR4Bi",
    "outputId": "3d221169-59eb-4788-de04-4205f1fb7809"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.298386812210083"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds  = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkZU6X2oSCWq",
    "outputId": "f51f76d6-2b69-435e-f4e7-5026a678d3dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct_pred(preds, labels) ## Correct predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmV0NiZmTQzO"
   },
   "source": [
    "## Train Using A Single Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-odUTehTTo9",
    "outputId": "2a212b16-3d55-4d48-d767-5da549beeae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.3126933574676514\n",
      "loss2: 2.2790932655334473\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) # Pass Batch\n",
    "loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "loss.backward() # Calculate Gradients\n",
    "optimizer.step() # Update Weights\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "print('loss1:', loss.item()) ##Loss before any optimization\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels) ##Loss after an optimization\n",
    "print('loss2:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_KaaIKYVAFN"
   },
   "source": [
    "## COMPLETE TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ogx1Ai0TnEF",
    "outputId": "f3fc0310-d4c4-408d-c9f4-f07144ec221a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x21629619b80>"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True) ## Turns on pytorch's gradient tracking feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "id": "nSZMlGjoqShu"
   },
   "outputs": [],
   "source": [
    "def get_num_correct_pred(preds,labels):\n",
    "  predicted_labels = preds.argmax(dim = 1)\n",
    "  comparision = predicted_labels.eq(labels)\n",
    "  num_correct_pred = comparision.sum().item()\n",
    "  return num_correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "id": "HGHxa8m5qTiS"
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root =  \"./data/FashinMNIST\", ## This tells it where to load the data from if its there\n",
    "    train = True, ## This tells we want the training set\n",
    "    download = True, ## This tells the class to download the data if its not present in the location we specified.\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])) ## Here we pass a composition of transformations that should be performed on the datasets\n",
    "                                                             ## We use ToTensor() transformation since we want our images to be transformed to tensors\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "      # (1) input layer\n",
    "      t = t \n",
    "\n",
    "      # (2) hidden conv layer\n",
    "      t = self.conv1(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (3) hidden conv layer\n",
    "      t = self.conv2(t)\n",
    "      t = F.relu(t)\n",
    "      t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "      # (4) hidden linear layer\n",
    "      t = t.reshape(-1, 12 * 4 * 4) # This means we want to reshape to (1x192).\n",
    "                                    # This flattens each image tensor in the batch\n",
    "                                    #This is a tensor of a batch of 1 and a flattened image pf 192 pixels\n",
    "      t = self.fc1(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (5) hidden linear layer\n",
    "      t = self.fc2(t)\n",
    "      t = F.relu(t)\n",
    "\n",
    "      # (6) output layer\n",
    "      t = self.out(t)\n",
    "  \n",
    "      return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-Hnwr9y-ZYq"
   },
   "source": [
    "## Single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WKEMaRsuT6j",
    "outputId": "f01189ff-2cd1-45c1-c009-a0f3f34e5fb6"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-510-8172426ad057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Get Batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m   \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Using all batches\n",
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "\n",
    "for batch in  train_loader: # Get Batches\n",
    "  images, labels = batch\n",
    "\n",
    "  preds = network(images) # Pass Batch\n",
    "  loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "  ##Total up the loss and num_correct predictions as we go\n",
    "  total_loss += loss.item()\n",
    "  total_correct += get_num_correct_pred(preds, label)\n",
    "\n",
    "  optimizer.zero_grad()## Zeros out any gradient values that currently exist.\n",
    "                      ## This is because pytorch accumulates gradients (grad++)\n",
    "  loss.backward() # Calculate Gradients\n",
    "  optimizer.step() # Update Weights\n",
    "\n",
    "print(\"Epoch:\", 0, \"Total correct:\", total_correct, \"Total loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGkhrKZZ-zJ5",
    "outputId": "94ec10e3-8f99-4255-8116-735aa5650120"
   },
   "outputs": [],
   "source": [
    "Accuracy = (total_correct/len(train_set))*100\n",
    "print(\"Accuracy % = \",Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6QLmEC6-tIY",
    "outputId": "71bba08a-ad41-40d9-bab2-ccf4447162b0"
   },
   "outputs": [],
   "source": [
    "## We can get the number of time the loop executes by calculating the number of batches\n",
    "num_batches = len(train_set)/100 #batch_size = 100\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iA6bousS-ewW"
   },
   "source": [
    "## Multiple **epochs**, complete training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KEK74xq-ow9",
    "outputId": "8768c01b-cfec-41f9-ccd7-f4ce7e19e560"
   },
   "outputs": [],
   "source": [
    "#Using all batches\n",
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for epoch in range(3):\n",
    "  for batch in  train_loader: # Get Batches\n",
    "    images, labels = batch\n",
    "\n",
    "    preds = network(images) # Pass Batch\n",
    "    loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "    ##Total up the loss and num_correct predictions as we go\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct_pred(preds, label)\n",
    "\n",
    "    optimizer.zero_grad()## Zeros out any gradient values that currently exist.\n",
    "                        ## This is because pytorch accumulates gradients (grad++)\n",
    "    loss.backward() # Calculate Gradients\n",
    "    optimizer.step() # Update Weights\n",
    "\n",
    "  Accuracy = (total_correct/len(train_set))*100 ## This is the accuracy for each epoch\n",
    "  print(\"Epoch:\",    epoch, \"Total correct:\",    total_correct, \"Total loss:\",    total_loss,    \"Accuracy\", Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlSglTtQz3yD",
    "outputId": "2f1a3dc8-4f20-409e-cd8f-2bf841dad4b5"
   },
   "outputs": [],
   "source": [
    "Accuracy = (total_correct/len(train_set))*100\n",
    "print(\"Accuracy % = \",Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoNm03GuAKrR"
   },
   "source": [
    "# Create A Confusion Matrix With PyTorch\n",
    "A confusion matrix will show us where the model is getting confused. To be more specific, the confusion matrix will show us which categories the model is predicting correctly and which categories the model is predicting incorrectly. For the incorrect predictions, we will be able to see which category the model predicted, and this will show us which categories are confusing the model.\n",
    "\n",
    "**To create a confusion matrix for our entire dataset, we need to have a prediction tensor with a single dimension that has the same length as our training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfoEylHLe6_d"
   },
   "source": [
    "## Get Predictions For The Entire Training Set\n",
    "To get the predictions for all the training set samples, we need to pass all of the samples forward through the network\n",
    "\n",
    "To do this, it is possible to create a DataLoader that has batch_size=1. This will pass a single batch to the network at once and will give us the desired prediction tensor for all the training set samples.\n",
    "\n",
    "However, depending on the computing resources and the size of the training set if we were training on a different data set, we need a way to prediction on smaller batches and collect the results. To collect the results, we'll use the torch.cat() function to concatenate the output tensors together to obtain our single prediction tensor. Let's build a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byNzn6sye-G1",
    "outputId": "a98c4fcb-1ac4-4375-ea78-b9dd1072cfeb"
   },
   "outputs": [],
   "source": [
    "#We'll create a function called get_all_preds(), and we'll pass a model and a data loader. The model will be used to obtain\n",
    "#the predictions, and the data loader will be used to provide the batches from the training set.\n",
    "#  All the function needs to do is iterate over the data loader passing the batches to the model\n",
    "#   and concatenating the results of each batch to a prediction tensor that will returned to the caller.\n",
    "\n",
    "@torch.no_grad() ## Because we want this functions execution to omit gradient tracking. This does it locally instead of globally(torch.set_grad_enabled(false)).\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat( (all_preds, preds), dim=0 )\n",
    "    return all_preds\n",
    "\n",
    "prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
    "train_preds = get_all_preds(network, prediction_loader)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Turning off PyTorch's gradient tracking feature locally in a different way\n",
    "#As another example we, we can use Python's with context manger keyword to specify that a specify block of code should exclude gradient computations.\n",
    "# with torch.no_grad():\n",
    "#     prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
    "#     train_preds = get_all_preds(network, prediction_loader)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vXyE6cJilsI",
    "outputId": "69ad54c3-9a9c-4663-8d24-3b9a599d7017"
   },
   "outputs": [],
   "source": [
    "train_preds.requires_grad #That's because we turned on gradient tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VziHZrfkjkjL"
   },
   "source": [
    "## Using The Predictions Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF6CzIFyggL7",
    "outputId": "925fddd2-ea3e-48d3-c179-d8e2c1e65ff9"
   },
   "outputs": [],
   "source": [
    "preds_correct = get_num_correct_pred(train_preds, train_set.targets)\n",
    "\n",
    "print('total correct:', preds_correct)\n",
    "print('accuracy:', preds_correct / len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ltdeaivl4LW"
   },
   "source": [
    "## Building a confusion maxtrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qW-ewJ6wj0kL",
    "outputId": "6e0181f2-dc2c-49aa-ef24-7770294066c9"
   },
   "outputs": [],
   "source": [
    "# To do this, we need to have the targets tensor and the predicted label from the train_preds tensor.\n",
    "\n",
    "train_set.targets\n",
    "train_preds.argmax(dim=1)\n",
    "\n",
    "print(train_set.targets)\n",
    "print(train_preds.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDV7Yl_qmEHW",
    "outputId": "e017f2c5-da32-4edf-c1df-b9977ed9387b"
   },
   "outputs": [],
   "source": [
    "# Now, if we compare the two tensors element-wise, \n",
    "# we can see if the predicted label matches the target. \n",
    "#We can pair up the two tensors by stacking them up\n",
    "\n",
    "stacked = torch.stack(  ( train_set.targets, train_preds.argmax(dim=1)), dim=1  )\n",
    "print(stacked)\n",
    "\n",
    "## We can get a list from the stacked tensor \n",
    "stacked[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nv-AL4Lnh9s",
    "outputId": "1ee4831a-2fa8-4f74-db9c-d0ecb2bb54f3"
   },
   "outputs": [],
   "source": [
    "# Now, we can iterate over these pairs and count the number of occurrences at each position in the matrix.\n",
    "# Let's create the matrix. Since we have ten prediction categories, we'll have a ten by ten matrix.\n",
    "# But first, we have to create an empty tenor which will eventually represent the confusion matrix\n",
    "cmt = torch.zeros(10,10, dtype=torch.int64)\n",
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zni19qmqoj-g"
   },
   "outputs": [],
   "source": [
    "# Now, we'll iterate over the prediction-target pairs and \n",
    "# add one to the value inside the matrix each time the particular position occurs.\n",
    "\n",
    "for p in stacked:\n",
    "    tl, pl = p.tolist()\n",
    "    cmt[tl, pl] = cmt[tl, pl] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CTjfrz3pC1b",
    "outputId": "3c80c613-c0d7-4b59-f382-dd835d2c5ab0"
   },
   "outputs": [],
   "source": [
    "cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm3ss6AHpN0m"
   },
   "source": [
    "## Plotting The Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCFDqnWYpPfG",
    "outputId": "647a0a1d-b16d-45ac-8822-3d88154df1b7"
   },
   "outputs": [],
   "source": [
    "# We can also use sklearn to generate the confusion matrix for us\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))\n",
    "cm ## As you can see, they look exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoXDCTOisoqa"
   },
   "outputs": [],
   "source": [
    "names = train_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "vGFPEEVYphKd",
    "outputId": "8d3bcf42-b7e3-4316-ec9a-9e5d3fa1d1fc"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.suptitle(\"Confusion matrix for our network as a heatmap\", fontsize = 30)\n",
    "plt.tick_params(axis='x', labelsize=15)#Make x-scale visible\n",
    "plt.tick_params(axis='y', labelsize=15)#Make y-scale visible\n",
    "plt.xticks(rotation = 45) # Rotate the classes alittle bit\n",
    "sns.heatmap(cmt, xticklabels = names, yticklabels = names, annot=True, fmt='d', cmap='copper_r')\n",
    "plt.xlabel(\"Predicted\", fontsize = 20)\n",
    "plt.ylabel(\"True\",  fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunnning\n",
    "\n",
    "We can manually change our parameters, but this is not an efficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "parameters = dict(\n",
    "    batch_size  =   [100, 1000, 10000],\n",
    "    rl          =  [0.00001, 0.0001, 0.001, 0.01],\n",
    "    shuffle     =  [True, False]    \n",
    ")\n",
    "\n",
    "\n",
    "## Alternative way of creating the dictionary\n",
    "# parameters = {\n",
    "#     'batch_size' :   [100, 1000, 10000],\n",
    "#     'rl'         :   [0.00001, 0.0001, 0.001, 0.01],\n",
    "#     'shuffle'    :   [True, False]    \n",
    "# }\n",
    "\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "param_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, rl, shuffle in product(*param_values):\n",
    "    print(batch_size,   rl,   shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for epoch in range(3):\n",
    "      for batch in  train_loader: # Get Batches\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        ##Total up the loss and num_correct predictions as we go\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct_pred(preds, label)\n",
    "\n",
    "        optimizer.zero_grad()## Zeros out any gradient values that currently exist.\n",
    "                            ## This is because pytorch accumulates gradients (grad++)\n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "    Accuracy = (total_correct/len(train_set))*100 ## This is the accuracy for each epoch\n",
    "    print(\"Epoch:\",    epoch, \"Total correct:\",    total_correct, \"Total loss:\",    total_loss,    \"Accuracy\", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Builder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunBuilder():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_runs(parameters):\n",
    "        Run = namedtuple('Run', parameters.keys())\n",
    "        \n",
    "        runs = []\n",
    "\n",
    "        for v in product(*parameters.values()):\n",
    "            runs.append(Run(*v))\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(batch_size=100, rl=1e-05, shuffle=True)\n",
      "Run(batch_size=100, rl=1e-05, shuffle=False)\n",
      "Run(batch_size=100, rl=0.0001, shuffle=True)\n",
      "Run(batch_size=100, rl=0.0001, shuffle=False)\n",
      "Run(batch_size=100, rl=0.001, shuffle=True)\n",
      "Run(batch_size=100, rl=0.001, shuffle=False)\n",
      "Run(batch_size=100, rl=0.01, shuffle=True)\n",
      "Run(batch_size=100, rl=0.01, shuffle=False)\n",
      "Run(batch_size=1000, rl=1e-05, shuffle=True)\n",
      "Run(batch_size=1000, rl=1e-05, shuffle=False)\n",
      "Run(batch_size=1000, rl=0.0001, shuffle=True)\n",
      "Run(batch_size=1000, rl=0.0001, shuffle=False)\n",
      "Run(batch_size=1000, rl=0.001, shuffle=True)\n",
      "Run(batch_size=1000, rl=0.001, shuffle=False)\n",
      "Run(batch_size=1000, rl=0.01, shuffle=True)\n",
      "Run(batch_size=1000, rl=0.01, shuffle=False)\n",
      "Run(batch_size=10000, rl=1e-05, shuffle=True)\n",
      "Run(batch_size=10000, rl=1e-05, shuffle=False)\n",
      "Run(batch_size=10000, rl=0.0001, shuffle=True)\n",
      "Run(batch_size=10000, rl=0.0001, shuffle=False)\n",
      "Run(batch_size=10000, rl=0.001, shuffle=True)\n",
      "Run(batch_size=10000, rl=0.001, shuffle=False)\n",
      "Run(batch_size=10000, rl=0.01, shuffle=True)\n",
      "Run(batch_size=10000, rl=0.01, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "## this is an example of generated runs by the RunBuilder class\n",
    "\n",
    "parameters = OrderedDict(\n",
    "    batch_size  =   [100, 1000, 10000],\n",
    "    rl          =  [0.00001, 0.0001, 0.001, 0.01],\n",
    "    shuffle     =  [True, False]    \n",
    ")\n",
    "\n",
    "for run in RunBuilder().get_runs(parameters):\n",
    "    comment = f'{run}'\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The RunManger For Training Loop Runs\n",
    "### We can try to experiment with different parameters manually, or we can build classes that will make experimentation very easy for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunManager():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "\n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "    \n",
    "    \n",
    "    def begin_run(self, run, network, loader):\n",
    "\n",
    "        self.run_start_time = time.time()\n",
    "\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "\n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "    #     self.tb = SummaryWriter(comment=f'-{run}')\n",
    "\n",
    "        images, labels = next(iter(self.loader))\n",
    "    #     grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    #     self.tb.add_image('images', grid)\n",
    "    #     self.tb.add_graph(\n",
    "    #             self.network\n",
    "    #         ,images.to(getattr(run, 'device', 'cpu'))\n",
    "\n",
    "\n",
    "\n",
    "    def end_run(self):\n",
    "    #     self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "\n",
    "\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "\n",
    "\n",
    "    def end_epoch(self):\n",
    "\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        loss = self.epoch_loss / len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "    #     self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "    #     self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "    #     for name, param in self.network.named_parameters():\n",
    "    #         self.tb.add_histogram(name, param, self.epoch_count)\n",
    "    #         self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "        results = OrderedDict()\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results['loss'] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results['epoch duration'] = epoch_duration\n",
    "        results['run duration'] = run_duration\n",
    "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "\n",
    "    def track_loss(self, loss):\n",
    "        self.epoch_loss += loss.item() * self.loader.batch_size#batch[0].shape[0]\n",
    "\n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self.get_num_correct(preds, labels)\n",
    "\n",
    "    def get_num_correct(self, preds, labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "    def save(self, fileName):\n",
    "\n",
    "        pd.DataFrame.from_dict(\n",
    "            self.run_data, orient='columns'\n",
    "        ).to_csv(f'{fileName}.csv')\n",
    "\n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our classes for Hyperparametr tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "      <th>num_workers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.014464</td>\n",
       "      <td>0.618567</td>\n",
       "      <td>19.738194</td>\n",
       "      <td>19.847848</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999374</td>\n",
       "      <td>0.619600</td>\n",
       "      <td>10.580713</td>\n",
       "      <td>17.105785</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.008386</td>\n",
       "      <td>0.615683</td>\n",
       "      <td>10.777055</td>\n",
       "      <td>12.089692</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934628</td>\n",
       "      <td>0.644350</td>\n",
       "      <td>11.313068</td>\n",
       "      <td>12.713879</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932536</td>\n",
       "      <td>0.641883</td>\n",
       "      <td>27.294381</td>\n",
       "      <td>27.510072</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.027729</td>\n",
       "      <td>0.602650</td>\n",
       "      <td>11.115199</td>\n",
       "      <td>12.318732</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936837</td>\n",
       "      <td>0.635083</td>\n",
       "      <td>10.437394</td>\n",
       "      <td>11.740575</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.973460</td>\n",
       "      <td>0.620983</td>\n",
       "      <td>10.800514</td>\n",
       "      <td>12.292872</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.266163</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>27.020365</td>\n",
       "      <td>27.331706</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.310304</td>\n",
       "      <td>0.530150</td>\n",
       "      <td>9.149822</td>\n",
       "      <td>10.508106</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.330327</td>\n",
       "      <td>0.490233</td>\n",
       "      <td>10.560747</td>\n",
       "      <td>12.088918</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.269592</td>\n",
       "      <td>0.512783</td>\n",
       "      <td>10.838140</td>\n",
       "      <td>12.551070</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.295470</td>\n",
       "      <td>0.502667</td>\n",
       "      <td>24.225823</td>\n",
       "      <td>24.651095</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1.260042</td>\n",
       "      <td>0.512483</td>\n",
       "      <td>9.765289</td>\n",
       "      <td>11.114024</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.308448</td>\n",
       "      <td>0.490767</td>\n",
       "      <td>9.607333</td>\n",
       "      <td>10.932203</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.271170</td>\n",
       "      <td>0.516183</td>\n",
       "      <td>10.972656</td>\n",
       "      <td>12.511623</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n",
       "0     1      1  1.014464  0.618567       19.738194     19.847848  0.01   \n",
       "1     2      1  0.999374  0.619600       10.580713     17.105785  0.01   \n",
       "2     3      1  1.008386  0.615683       10.777055     12.089692  0.01   \n",
       "3     4      1  0.934628  0.644350       11.313068     12.713879  0.01   \n",
       "4     5      1  0.932536  0.641883       27.294381     27.510072  0.01   \n",
       "5     6      1  1.027729  0.602650       11.115199     12.318732  0.01   \n",
       "6     7      1  0.936837  0.635083       10.437394     11.740575  0.01   \n",
       "7     8      1  0.973460  0.620983       10.800514     12.292872  0.01   \n",
       "8     9      1  1.266163  0.523700       27.020365     27.331706  0.01   \n",
       "9    10      1  1.310304  0.530150        9.149822     10.508106  0.01   \n",
       "10   11      1  1.330327  0.490233       10.560747     12.088918  0.01   \n",
       "11   12      1  1.269592  0.512783       10.838140     12.551070  0.01   \n",
       "12   13      1  1.295470  0.502667       24.225823     24.651095  0.01   \n",
       "13   14      1  1.260042  0.512483        9.765289     11.114024  0.01   \n",
       "14   15      1  1.308448  0.490767        9.607333     10.932203  0.01   \n",
       "15   16      1  1.271170  0.516183       10.972656     12.511623  0.01   \n",
       "\n",
       "    batch_size  shuffle  num_workers  \n",
       "0         1000     True            0  \n",
       "1         1000     True            1  \n",
       "2         1000     True            2  \n",
       "3         1000     True            4  \n",
       "4         1000    False            0  \n",
       "5         1000    False            1  \n",
       "6         1000    False            2  \n",
       "7         1000    False            4  \n",
       "8         2000     True            0  \n",
       "9         2000     True            1  \n",
       "10        2000     True            2  \n",
       "11        2000     True            4  \n",
       "12        2000    False            0  \n",
       "13        2000    False            1  \n",
       "14        2000    False            2  \n",
       "15        2000    False            4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = OrderedDict(\n",
    "    lr = [.01]\n",
    "    ,batch_size = [1000, 2000]\n",
    "    ,shuffle = [True, False],\n",
    "    num_workers = [0, 1, 2, 4]\n",
    ")\n",
    "\n",
    "\n",
    "m = RunManager()\n",
    "for run in RunBuilder.get_runs(parameters):\n",
    "    network = Network()\n",
    "    loader = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle, num_workers=run.num_workers)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "    \n",
    "    m.begin_run(run, network,loader)\n",
    "    for epoch in range(1):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            \n",
    "            images, labels =  batch\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(preds, labels)\n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save('results')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch and the GPU: CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the image tensor and instatiatying the network\n",
    "t = torch.ones(1,1,28, 28)\n",
    "network = Network()\n",
    "\n",
    "## Moving the image tensor and the network to the GPU\n",
    "t = t.cuda()\n",
    "network = network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_pred = network(t)\n",
    "gpu_pred.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.cpu()\n",
    "network = network.cpu()\n",
    "\n",
    "cpu_pred = network(t)\n",
    "cpu_pred.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations with tensors on different devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2,3], [2,3,5]])\n",
    "t2 = torch.tensor([[6,7,8], [9,10,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.device ## By default, Pytorch creates the tensors on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.device ## By default, Pytorch creates the tensors on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = t1.to('cuda') # Moving t1 to the GPU\n",
    "\n",
    "## Or t1 = t1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    t1 + t2\n",
    "except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's move t2 to cuda as well\n",
    "t2 = t2.cuda()\n",
    "\n",
    "try:\n",
    "    T = t1 + t2\n",
    "except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "## Now as you can see, it works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch nn.Module Computations On A GPU\n",
    "By deafult, all network parameters are on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu  conv1.weight\n",
      "cpu  conv1.bias\n",
      "cpu  conv2.weight\n",
      "cpu  conv2.bias\n",
      "cpu  fc1.weight\n",
      "cpu  fc1.bias\n",
      "cpu  fc2.weight\n",
      "cpu  fc2.bias\n",
      "cpu  out.weight\n",
      "cpu  out.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in network.named_parameters():\n",
    "    print(p.device, '', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what happens when we ask a network to be moved to() the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0  conv1.weight\n",
      "cuda:0  conv1.bias\n",
      "cuda:0  conv2.weight\n",
      "cuda:0  conv2.bias\n",
      "cuda:0  fc1.weight\n",
      "cuda:0  fc1.bias\n",
      "cuda:0  fc2.weight\n",
      "cuda:0  fc2.bias\n",
      "cuda:0  out.weight\n",
      "cuda:0  out.bias\n"
     ]
    }
   ],
   "source": [
    "# we can see that all parameters have been moved to the GPU\n",
    "for n, p in network.named_parameters():\n",
    "    print(p.device, '', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing A Sample To The Network\n",
    "Here pytorch complains since the network is on a GPU and the tensor is on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\n"
     ]
    }
   ],
   "source": [
    "sample = torch.ones(1,1,28,28)\n",
    "\n",
    "try:\n",
    "    network(sample)\n",
    "except Exception as e: \n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can fix this issue by sending our sample to the GPU like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1229,  0.0830, -0.1051, -0.1133, -0.0103, -0.0654, -0.0424, -0.0980,  0.0735,  0.1214]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pred = network(sample.to('cuda'))\n",
    "    print(pred)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Device Agnostic PyTorch Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This term device agnostic means that our code doesn't depend on the underlying device. You may come across this terminology when reading PyTorch documentation.\n",
    "\n",
    "For example, suppose we write code that uses the cuda() method everywhere, and then, we give the code to a user who doesn't have a GPU. This won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's repeat our traning loop including the device attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.002382</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>10.750423</td>\n",
       "      <td>10.852416</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944975</td>\n",
       "      <td>0.634767</td>\n",
       "      <td>5.578809</td>\n",
       "      <td>5.665811</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975558</td>\n",
       "      <td>0.628850</td>\n",
       "      <td>7.234999</td>\n",
       "      <td>8.147615</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.012713</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>5.240998</td>\n",
       "      <td>6.252990</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946269</td>\n",
       "      <td>0.636517</td>\n",
       "      <td>8.346081</td>\n",
       "      <td>9.524083</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891226</td>\n",
       "      <td>0.671550</td>\n",
       "      <td>3.737000</td>\n",
       "      <td>4.928998</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966517</td>\n",
       "      <td>0.623800</td>\n",
       "      <td>8.927147</td>\n",
       "      <td>10.425147</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.947105</td>\n",
       "      <td>0.636533</td>\n",
       "      <td>3.633596</td>\n",
       "      <td>5.369637</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.014182</td>\n",
       "      <td>0.604750</td>\n",
       "      <td>12.267548</td>\n",
       "      <td>12.361547</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995647</td>\n",
       "      <td>0.621317</td>\n",
       "      <td>6.313048</td>\n",
       "      <td>6.437048</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991838</td>\n",
       "      <td>0.627283</td>\n",
       "      <td>8.858442</td>\n",
       "      <td>9.774304</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.028368</td>\n",
       "      <td>0.606533</td>\n",
       "      <td>5.326755</td>\n",
       "      <td>6.341788</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998483</td>\n",
       "      <td>0.622283</td>\n",
       "      <td>8.798585</td>\n",
       "      <td>9.901122</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1.009813</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>3.883980</td>\n",
       "      <td>5.047061</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918188</td>\n",
       "      <td>0.652367</td>\n",
       "      <td>9.783637</td>\n",
       "      <td>11.419155</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.052663</td>\n",
       "      <td>0.588533</td>\n",
       "      <td>3.746357</td>\n",
       "      <td>5.441325</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1.454884</td>\n",
       "      <td>0.446033</td>\n",
       "      <td>12.866938</td>\n",
       "      <td>13.074975</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1.322968</td>\n",
       "      <td>0.507667</td>\n",
       "      <td>5.953417</td>\n",
       "      <td>6.165416</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1.237006</td>\n",
       "      <td>0.541383</td>\n",
       "      <td>9.363252</td>\n",
       "      <td>10.658288</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.402347</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>5.823616</td>\n",
       "      <td>7.084589</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.393764</td>\n",
       "      <td>0.479317</td>\n",
       "      <td>9.462038</td>\n",
       "      <td>10.849666</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.361885</td>\n",
       "      <td>0.483833</td>\n",
       "      <td>4.325604</td>\n",
       "      <td>6.119606</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.500467</td>\n",
       "      <td>10.576031</td>\n",
       "      <td>12.739916</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.260539</td>\n",
       "      <td>0.528650</td>\n",
       "      <td>4.197913</td>\n",
       "      <td>6.468914</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.264380</td>\n",
       "      <td>0.518550</td>\n",
       "      <td>13.701995</td>\n",
       "      <td>13.900994</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.380919</td>\n",
       "      <td>0.486783</td>\n",
       "      <td>6.481085</td>\n",
       "      <td>6.709098</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1.271377</td>\n",
       "      <td>0.518367</td>\n",
       "      <td>10.256521</td>\n",
       "      <td>11.608040</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1.543085</td>\n",
       "      <td>0.406417</td>\n",
       "      <td>7.006601</td>\n",
       "      <td>8.590600</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1.338648</td>\n",
       "      <td>0.497550</td>\n",
       "      <td>10.903389</td>\n",
       "      <td>12.554370</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1.180219</td>\n",
       "      <td>0.556317</td>\n",
       "      <td>5.027003</td>\n",
       "      <td>6.982525</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.266655</td>\n",
       "      <td>0.520983</td>\n",
       "      <td>10.991905</td>\n",
       "      <td>13.167907</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.382579</td>\n",
       "      <td>0.481133</td>\n",
       "      <td>4.362524</td>\n",
       "      <td>6.663525</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n",
       "0     1      1  1.002382  0.616900       10.750423     10.852416  0.01   \n",
       "1     2      1  0.944975  0.634767        5.578809      5.665811  0.01   \n",
       "2     3      1  0.975558  0.628850        7.234999      8.147615  0.01   \n",
       "3     4      1  1.012713  0.602800        5.240998      6.252990  0.01   \n",
       "4     5      1  0.946269  0.636517        8.346081      9.524083  0.01   \n",
       "5     6      1  0.891226  0.671550        3.737000      4.928998  0.01   \n",
       "6     7      1  0.966517  0.623800        8.927147     10.425147  0.01   \n",
       "7     8      1  0.947105  0.636533        3.633596      5.369637  0.01   \n",
       "8     9      1  1.014182  0.604750       12.267548     12.361547  0.01   \n",
       "9    10      1  0.995647  0.621317        6.313048      6.437048  0.01   \n",
       "10   11      1  0.991838  0.627283        8.858442      9.774304  0.01   \n",
       "11   12      1  1.028368  0.606533        5.326755      6.341788  0.01   \n",
       "12   13      1  0.998483  0.622283        8.798585      9.901122  0.01   \n",
       "13   14      1  1.009813  0.619400        3.883980      5.047061  0.01   \n",
       "14   15      1  0.918188  0.652367        9.783637     11.419155  0.01   \n",
       "15   16      1  1.052663  0.588533        3.746357      5.441325  0.01   \n",
       "16   17      1  1.454884  0.446033       12.866938     13.074975  0.01   \n",
       "17   18      1  1.322968  0.507667        5.953417      6.165416  0.01   \n",
       "18   19      1  1.237006  0.541383        9.363252     10.658288  0.01   \n",
       "19   20      1  1.402347  0.467333        5.823616      7.084589  0.01   \n",
       "20   21      1  1.393764  0.479317        9.462038     10.849666  0.01   \n",
       "21   22      1  1.361885  0.483833        4.325604      6.119606  0.01   \n",
       "22   23      1  1.393889  0.500467       10.576031     12.739916  0.01   \n",
       "23   24      1  1.260539  0.528650        4.197913      6.468914  0.01   \n",
       "24   25      1  1.264380  0.518550       13.701995     13.900994  0.01   \n",
       "25   26      1  1.380919  0.486783        6.481085      6.709098  0.01   \n",
       "26   27      1  1.271377  0.518367       10.256521     11.608040  0.01   \n",
       "27   28      1  1.543085  0.406417        7.006601      8.590600  0.01   \n",
       "28   29      1  1.338648  0.497550       10.903389     12.554370  0.01   \n",
       "29   30      1  1.180219  0.556317        5.027003      6.982525  0.01   \n",
       "30   31      1  1.266655  0.520983       10.991905     13.167907  0.01   \n",
       "31   32      1  1.382579  0.481133        4.362524      6.663525  0.01   \n",
       "\n",
       "    batch_size  shuffle  num_workers device  \n",
       "0         1000     True            0    cpu  \n",
       "1         1000     True            0   cuda  \n",
       "2         1000     True            1    cpu  \n",
       "3         1000     True            1   cuda  \n",
       "4         1000     True            2    cpu  \n",
       "5         1000     True            2   cuda  \n",
       "6         1000     True            4    cpu  \n",
       "7         1000     True            4   cuda  \n",
       "8         1000    False            0    cpu  \n",
       "9         1000    False            0   cuda  \n",
       "10        1000    False            1    cpu  \n",
       "11        1000    False            1   cuda  \n",
       "12        1000    False            2    cpu  \n",
       "13        1000    False            2   cuda  \n",
       "14        1000    False            4    cpu  \n",
       "15        1000    False            4   cuda  \n",
       "16        2000     True            0    cpu  \n",
       "17        2000     True            0   cuda  \n",
       "18        2000     True            1    cpu  \n",
       "19        2000     True            1   cuda  \n",
       "20        2000     True            2    cpu  \n",
       "21        2000     True            2   cuda  \n",
       "22        2000     True            4    cpu  \n",
       "23        2000     True            4   cuda  \n",
       "24        2000    False            0    cpu  \n",
       "25        2000    False            0   cuda  \n",
       "26        2000    False            1    cpu  \n",
       "27        2000    False            1   cuda  \n",
       "28        2000    False            2    cpu  \n",
       "29        2000    False            2   cuda  \n",
       "30        2000    False            4    cpu  \n",
       "31        2000    False            4   cuda  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = OrderedDict(\n",
    "    lr = [.01]\n",
    "    ,batch_size = [1000, 2000]\n",
    "    ,shuffle = [True, False],\n",
    "    num_workers = [0, 1, 2, 4],\n",
    "    device = ['cpu', 'cuda']\n",
    ")\n",
    "\n",
    "\n",
    "m = RunManager()\n",
    "for run in RunBuilder.get_runs(parameters):\n",
    "    \n",
    "    device = torch.device(run.device)\n",
    "    \n",
    "    network = Network()\n",
    "    network.to(device) ## Move our network to the current device\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle, num_workers=run.num_workers)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "    \n",
    "    m.begin_run(run, network,loader)\n",
    "    for epoch in range(1):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            \n",
    "            images, labels =  batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(preds, labels)\n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save('results')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the results according to epoch duration and we see cuda is winning in every case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1.543085</td>\n",
       "      <td>0.406417</td>\n",
       "      <td>7.006601</td>\n",
       "      <td>8.590600</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1.454884</td>\n",
       "      <td>0.446033</td>\n",
       "      <td>12.866938</td>\n",
       "      <td>13.074975</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.402347</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>5.823616</td>\n",
       "      <td>7.084589</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.393764</td>\n",
       "      <td>0.479317</td>\n",
       "      <td>9.462038</td>\n",
       "      <td>10.849666</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.382579</td>\n",
       "      <td>0.481133</td>\n",
       "      <td>4.362524</td>\n",
       "      <td>6.663525</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.361885</td>\n",
       "      <td>0.483833</td>\n",
       "      <td>4.325604</td>\n",
       "      <td>6.119606</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.380919</td>\n",
       "      <td>0.486783</td>\n",
       "      <td>6.481085</td>\n",
       "      <td>6.709098</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1.338648</td>\n",
       "      <td>0.497550</td>\n",
       "      <td>10.903389</td>\n",
       "      <td>12.554370</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.500467</td>\n",
       "      <td>10.576031</td>\n",
       "      <td>12.739916</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1.322968</td>\n",
       "      <td>0.507667</td>\n",
       "      <td>5.953417</td>\n",
       "      <td>6.165416</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1.271377</td>\n",
       "      <td>0.518367</td>\n",
       "      <td>10.256521</td>\n",
       "      <td>11.608040</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.264380</td>\n",
       "      <td>0.518550</td>\n",
       "      <td>13.701995</td>\n",
       "      <td>13.900994</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.266655</td>\n",
       "      <td>0.520983</td>\n",
       "      <td>10.991905</td>\n",
       "      <td>13.167907</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.260539</td>\n",
       "      <td>0.528650</td>\n",
       "      <td>4.197913</td>\n",
       "      <td>6.468914</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1.237006</td>\n",
       "      <td>0.541383</td>\n",
       "      <td>9.363252</td>\n",
       "      <td>10.658288</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1.180219</td>\n",
       "      <td>0.556317</td>\n",
       "      <td>5.027003</td>\n",
       "      <td>6.982525</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.052663</td>\n",
       "      <td>0.588533</td>\n",
       "      <td>3.746357</td>\n",
       "      <td>5.441325</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.012713</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>5.240998</td>\n",
       "      <td>6.252990</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.014182</td>\n",
       "      <td>0.604750</td>\n",
       "      <td>12.267548</td>\n",
       "      <td>12.361547</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.028368</td>\n",
       "      <td>0.606533</td>\n",
       "      <td>5.326755</td>\n",
       "      <td>6.341788</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.002382</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>10.750423</td>\n",
       "      <td>10.852416</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1.009813</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>3.883980</td>\n",
       "      <td>5.047061</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995647</td>\n",
       "      <td>0.621317</td>\n",
       "      <td>6.313048</td>\n",
       "      <td>6.437048</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998483</td>\n",
       "      <td>0.622283</td>\n",
       "      <td>8.798585</td>\n",
       "      <td>9.901122</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966517</td>\n",
       "      <td>0.623800</td>\n",
       "      <td>8.927147</td>\n",
       "      <td>10.425147</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991838</td>\n",
       "      <td>0.627283</td>\n",
       "      <td>8.858442</td>\n",
       "      <td>9.774304</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975558</td>\n",
       "      <td>0.628850</td>\n",
       "      <td>7.234999</td>\n",
       "      <td>8.147615</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944975</td>\n",
       "      <td>0.634767</td>\n",
       "      <td>5.578809</td>\n",
       "      <td>5.665811</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946269</td>\n",
       "      <td>0.636517</td>\n",
       "      <td>8.346081</td>\n",
       "      <td>9.524083</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.947105</td>\n",
       "      <td>0.636533</td>\n",
       "      <td>3.633596</td>\n",
       "      <td>5.369637</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918188</td>\n",
       "      <td>0.652367</td>\n",
       "      <td>9.783637</td>\n",
       "      <td>11.419155</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891226</td>\n",
       "      <td>0.671550</td>\n",
       "      <td>3.737000</td>\n",
       "      <td>4.928998</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n",
       "27   28      1  1.543085  0.406417        7.006601      8.590600  0.01   \n",
       "16   17      1  1.454884  0.446033       12.866938     13.074975  0.01   \n",
       "19   20      1  1.402347  0.467333        5.823616      7.084589  0.01   \n",
       "20   21      1  1.393764  0.479317        9.462038     10.849666  0.01   \n",
       "31   32      1  1.382579  0.481133        4.362524      6.663525  0.01   \n",
       "21   22      1  1.361885  0.483833        4.325604      6.119606  0.01   \n",
       "25   26      1  1.380919  0.486783        6.481085      6.709098  0.01   \n",
       "28   29      1  1.338648  0.497550       10.903389     12.554370  0.01   \n",
       "22   23      1  1.393889  0.500467       10.576031     12.739916  0.01   \n",
       "17   18      1  1.322968  0.507667        5.953417      6.165416  0.01   \n",
       "26   27      1  1.271377  0.518367       10.256521     11.608040  0.01   \n",
       "24   25      1  1.264380  0.518550       13.701995     13.900994  0.01   \n",
       "30   31      1  1.266655  0.520983       10.991905     13.167907  0.01   \n",
       "23   24      1  1.260539  0.528650        4.197913      6.468914  0.01   \n",
       "18   19      1  1.237006  0.541383        9.363252     10.658288  0.01   \n",
       "29   30      1  1.180219  0.556317        5.027003      6.982525  0.01   \n",
       "15   16      1  1.052663  0.588533        3.746357      5.441325  0.01   \n",
       "3     4      1  1.012713  0.602800        5.240998      6.252990  0.01   \n",
       "8     9      1  1.014182  0.604750       12.267548     12.361547  0.01   \n",
       "11   12      1  1.028368  0.606533        5.326755      6.341788  0.01   \n",
       "0     1      1  1.002382  0.616900       10.750423     10.852416  0.01   \n",
       "13   14      1  1.009813  0.619400        3.883980      5.047061  0.01   \n",
       "9    10      1  0.995647  0.621317        6.313048      6.437048  0.01   \n",
       "12   13      1  0.998483  0.622283        8.798585      9.901122  0.01   \n",
       "6     7      1  0.966517  0.623800        8.927147     10.425147  0.01   \n",
       "10   11      1  0.991838  0.627283        8.858442      9.774304  0.01   \n",
       "2     3      1  0.975558  0.628850        7.234999      8.147615  0.01   \n",
       "1     2      1  0.944975  0.634767        5.578809      5.665811  0.01   \n",
       "4     5      1  0.946269  0.636517        8.346081      9.524083  0.01   \n",
       "7     8      1  0.947105  0.636533        3.633596      5.369637  0.01   \n",
       "14   15      1  0.918188  0.652367        9.783637     11.419155  0.01   \n",
       "5     6      1  0.891226  0.671550        3.737000      4.928998  0.01   \n",
       "\n",
       "    batch_size  shuffle  num_workers device  \n",
       "27        2000    False            1   cuda  \n",
       "16        2000     True            0    cpu  \n",
       "19        2000     True            1   cuda  \n",
       "20        2000     True            2    cpu  \n",
       "31        2000    False            4   cuda  \n",
       "21        2000     True            2   cuda  \n",
       "25        2000    False            0   cuda  \n",
       "28        2000    False            2    cpu  \n",
       "22        2000     True            4    cpu  \n",
       "17        2000     True            0   cuda  \n",
       "26        2000    False            1    cpu  \n",
       "24        2000    False            0    cpu  \n",
       "30        2000    False            4    cpu  \n",
       "23        2000     True            4   cuda  \n",
       "18        2000     True            1    cpu  \n",
       "29        2000    False            2   cuda  \n",
       "15        1000    False            4   cuda  \n",
       "3         1000     True            1   cuda  \n",
       "8         1000    False            0    cpu  \n",
       "11        1000    False            1   cuda  \n",
       "0         1000     True            0    cpu  \n",
       "13        1000    False            2   cuda  \n",
       "9         1000    False            0   cuda  \n",
       "12        1000    False            2    cpu  \n",
       "6         1000     True            4    cpu  \n",
       "10        1000    False            1    cpu  \n",
       "2         1000     True            1    cpu  \n",
       "1         1000     True            0   cuda  \n",
       "4         1000     True            2    cpu  \n",
       "7         1000     True            4   cuda  \n",
       "14        1000    False            4    cpu  \n",
       "5         1000     True            2   cuda  "
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(m.run_data, orient = 'columns').sort_values('epoch duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BXrhgTjFG0uz",
    "pIRtFQXsk4kE",
    "w6tvopWYR4VU"
   ],
   "name": "Fashion -MNIST",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ac14e06fc2f4793ad44354e6c709499": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c778670e477433f8b2af7cfde745da8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e81809797e64e5b811e3961e2846655": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f42157c5fb44849a78d12bff18b9f7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e81809797e64e5b811e3961e2846655",
      "max": 5148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f51cee8f47c144d68f4b35240a3339d9",
      "value": 5148
     }
    },
    "21bb443e7ff6459bad933287f03e98d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f42157c5fb44849a78d12bff18b9f7a",
       "IPY_MODEL_6b76d137b75e4344b0b4d49f27fca9b9"
      ],
      "layout": "IPY_MODEL_3d148d4d57ff4298920daf031b3fa78d"
     }
    },
    "2416d93c5f0c4437b1218c27aaf8cf17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d148d4d57ff4298920daf031b3fa78d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f01e2e92edf4ed2a427e6a985e3e921": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bd0e72cf03d4ef99ff83fc3840c6d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "583b6c6f7340488e8c85c0eb1fd89d23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58eeb471f15445c8b3a274b9c3aaaf2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "68c6acfdc844472c9823350064eee16d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b76d137b75e4344b0b4d49f27fca9b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97302b7800c44da5a26eb685666f4b1c",
      "placeholder": "​",
      "style": "IPY_MODEL_3f01e2e92edf4ed2a427e6a985e3e921",
      "value": " 6144/? [00:00&lt;00:00, 25490.28it/s]"
     }
    },
    "70fda96397bf403197f1d14851f5d956": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ac14e06fc2f4793ad44354e6c709499",
      "placeholder": "​",
      "style": "IPY_MODEL_d3c18772dc724e8ebf714ff686af3155",
      "value": " 4422656/? [00:01&lt;00:00, 2429536.91it/s]"
     }
    },
    "744cd03be9d7438cbc173479aaa6f08f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74be6a0d694541e983973f44e486610b",
       "IPY_MODEL_88832844280c4fd2a1953651b33069f2"
      ],
      "layout": "IPY_MODEL_d687d8fac8ca4d82922f14a81cefad27"
     }
    },
    "74be6a0d694541e983973f44e486610b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c778670e477433f8b2af7cfde745da8",
      "max": 29515,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c636de35adb74d0f9325db09301551df",
      "value": 29515
     }
    },
    "7ae21dccb1b74efaa75fc0c7f4139972": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1a39a61d3a94bfc9aacba6e54ae41c2",
       "IPY_MODEL_70fda96397bf403197f1d14851f5d956"
      ],
      "layout": "IPY_MODEL_a42b4a914da742109430f68d14983df5"
     }
    },
    "8565bdaceafe4951ab39751e7d0a2793": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddfdedafa206448a9f4023ee8715fdec",
      "placeholder": "​",
      "style": "IPY_MODEL_583b6c6f7340488e8c85c0eb1fd89d23",
      "value": " 26422272/? [00:04&lt;00:00, 5520634.07it/s]"
     }
    },
    "88832844280c4fd2a1953651b33069f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2416d93c5f0c4437b1218c27aaf8cf17",
      "placeholder": "​",
      "style": "IPY_MODEL_4bd0e72cf03d4ef99ff83fc3840c6d80",
      "value": " 29696/? [00:00&lt;00:00, 56346.27it/s]"
     }
    },
    "97302b7800c44da5a26eb685666f4b1c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "988c2ca3586c4f149cf8387c71d8aa2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68c6acfdc844472c9823350064eee16d",
      "max": 26421880,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_58eeb471f15445c8b3a274b9c3aaaf2f",
      "value": 26421880
     }
    },
    "a42b4a914da742109430f68d14983df5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b48293faed504d58a1c9a38d6fd8bfef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b665362b662c48bf8167a02a888f5927": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbe76057e82641c5a1ed6f7d904d4751": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_988c2ca3586c4f149cf8387c71d8aa2b",
       "IPY_MODEL_8565bdaceafe4951ab39751e7d0a2793"
      ],
      "layout": "IPY_MODEL_b665362b662c48bf8167a02a888f5927"
     }
    },
    "c1a39a61d3a94bfc9aacba6e54ae41c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b48293faed504d58a1c9a38d6fd8bfef",
      "max": 4422102,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de47dcbe7cd14f53beb10c068683ee19",
      "value": 4422102
     }
    },
    "c636de35adb74d0f9325db09301551df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d3c18772dc724e8ebf714ff686af3155": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d687d8fac8ca4d82922f14a81cefad27": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddfdedafa206448a9f4023ee8715fdec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de47dcbe7cd14f53beb10c068683ee19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f51cee8f47c144d68f4b35240a3339d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
